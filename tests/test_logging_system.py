"""
ë¡œê¹… ë° ì¤‘ì•™ì§‘ì¤‘ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ TDD í…ŒìŠ¤íŠ¸

êµ¬ì¡°í™”ëœ ë¡œê¹…, ì¤‘ì•™ì§‘ì¤‘ì‹ ë¡œê·¸ ê´€ë¦¬, ë¡œê·¸ ìˆ˜ì§‘ ë° ë¶„ì„ì„ ìœ„í•œ TDD í…ŒìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""
import json
import uuid
from datetime import datetime
from typing import Any, Dict, List


class TestStructuredLogging:
    """êµ¬ì¡°í™”ëœ ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""

    def test_structured_log_format(self):
        """êµ¬ì¡°í™”ëœ ë¡œê·¸ í¬ë§· í…ŒìŠ¤íŠ¸"""
        # Given: êµ¬ì¡°í™”ëœ ë¡œê±° ì„¤ì •
        structured_logs = []

        def create_structured_log(
            level: str,
            message: str,
            trace_id: str = None,
            user_id: str = None,
            extra_fields: Dict[str, Any] = None,
        ) -> Dict[str, Any]:
            """êµ¬ì¡°í™”ëœ ë¡œê·¸ ìƒì„±"""
            log_entry = {
                "timestamp": datetime.now().isoformat(),
                "level": level.upper(),
                "message": message,
                "service": "hotly-app",
                "version": "1.0.0",
            }

            if trace_id:
                log_entry["trace_id"] = trace_id
            if user_id:
                log_entry["user_id"] = user_id
            if extra_fields:
                log_entry.update(extra_fields)

            structured_logs.append(log_entry)
            return log_entry

        # When: ë‹¤ì–‘í•œ ë ˆë²¨ì˜ ë¡œê·¸ ìƒì„±
        info_log = create_structured_log(
            level="info",
            message="User login successful",
            trace_id="trace-123",
            user_id="user-456",
            extra_fields={"login_method": "google", "ip_address": "192.168.1.1"},
        )

        error_log = create_structured_log(
            level="error",
            message="Database connection failed",
            trace_id="trace-124",
            extra_fields={"database": "postgresql", "timeout": 30},
        )

        # Then: ë¡œê·¸ êµ¬ì¡° ê²€ì¦
        assert info_log["level"] == "INFO"
        assert info_log["message"] == "User login successful"
        assert info_log["trace_id"] == "trace-123"
        assert info_log["user_id"] == "user-456"
        assert info_log["service"] == "hotly-app"
        assert info_log["login_method"] == "google"
        assert "timestamp" in info_log

        assert error_log["level"] == "ERROR"
        assert error_log["database"] == "postgresql"
        assert "user_id" not in error_log  # ì„¤ì •ë˜ì§€ ì•ŠìŒ

        # JSON ì§ë ¬í™” ê°€ëŠ¥í•œì§€ í™•ì¸
        json_log = json.dumps(info_log)
        assert json_log is not None

        print("âœ… êµ¬ì¡°í™”ëœ ë¡œê·¸ í¬ë§· í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_sensitive_data_masking(self):
        """ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸"""

        # Given: ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹ í•¨ìˆ˜
        def mask_sensitive_data(data: Dict[str, Any]) -> Dict[str, Any]:
            """ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹"""
            masked_data = data.copy()
            sensitive_fields = ["password", "token", "api_key", "credit_card"]

            for field in sensitive_fields:
                if field in masked_data:
                    value = str(masked_data[field])
                    if len(value) > 4:
                        masked_data[field] = f"{value[:2]}***{value[-2:]}"
                    else:
                        masked_data[field] = "***"

            # ì¤‘ì²©ëœ ë”•ì…”ë„ˆë¦¬ë„ ì²˜ë¦¬
            for key, value in masked_data.items():
                if isinstance(value, dict):
                    masked_data[key] = mask_sensitive_data(value)

            return masked_data

        # When: ë¯¼ê°ì •ë³´ê°€ í¬í•¨ëœ ë¡œê·¸ ë°ì´í„°
        sensitive_log = {
            "user_id": "user-123",
            "password": "secretpassword123",
            "token": "jwt-token-abcdef12345",
            "api_key": "sk-abc123def456ghi789",
            "normal_field": "normal_value",
            "user_info": {"email": "user@example.com", "password": "nested_password"},
        }

        masked_log = mask_sensitive_data(sensitive_log)

        # Then: ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹ í™•ì¸
        assert masked_log["password"] == "se***23"
        assert masked_log["token"] == "jw***45"
        assert masked_log["api_key"] == "sk***89"
        assert masked_log["normal_field"] == "normal_value"  # ë³€ê²½ë˜ì§€ ì•ŠìŒ
        assert masked_log["user_info"]["password"] == "ne***rd"
        assert masked_log["user_info"]["email"] == "user@example.com"  # ë³€ê²½ë˜ì§€ ì•ŠìŒ

        print("âœ… ë¯¼ê°ì •ë³´ ë§ˆìŠ¤í‚¹ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_log_correlation_tracing(self):
        """ë¡œê·¸ ìƒê´€ê´€ê³„ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        # Given: ìš”ì²­ ì¶”ì  ì‹œìŠ¤í…œ
        request_traces = {}

        def start_trace(request_id: str) -> str:
            """ìš”ì²­ ì¶”ì  ì‹œì‘"""
            trace_id = f"trace-{uuid.uuid4().hex[:8]}"
            request_traces[trace_id] = {
                "request_id": request_id,
                "start_time": datetime.now(),
                "logs": [],
                "status": "active",
            }
            return trace_id

        def add_trace_log(trace_id: str, level: str, message: str, extra: Dict = None):
            """ì¶”ì ì— ë¡œê·¸ ì¶”ê°€"""
            if trace_id in request_traces:
                log_entry = {
                    "timestamp": datetime.now(),
                    "level": level,
                    "message": message,
                    **(extra or {}),
                }
                request_traces[trace_id]["logs"].append(log_entry)

        def end_trace(trace_id: str, status: str = "completed"):
            """ìš”ì²­ ì¶”ì  ì¢…ë£Œ"""
            if trace_id in request_traces:
                request_traces[trace_id]["status"] = status
                request_traces[trace_id]["end_time"] = datetime.now()
                request_traces[trace_id]["duration_ms"] = (
                    request_traces[trace_id]["end_time"]
                    - request_traces[trace_id]["start_time"]
                ).total_seconds() * 1000

        # When: ìš”ì²­ ì²˜ë¦¬ í”Œë¡œìš° ì‹œë®¬ë ˆì´ì…˜
        trace_id = start_trace("req-12345")

        add_trace_log(
            trace_id, "INFO", "Request started", {"endpoint": "/api/v1/places"}
        )
        add_trace_log(
            trace_id, "DEBUG", "Database query executed", {"query_time_ms": 45}
        )
        add_trace_log(trace_id, "INFO", "Cache miss", {"cache_key": "places:user-123"})
        add_trace_log(trace_id, "INFO", "Request completed", {"status_code": 200})

        end_trace(trace_id, "completed")

        # Then: ì¶”ì  ì •ë³´ ê²€ì¦
        trace = request_traces[trace_id]
        assert trace["request_id"] == "req-12345"
        assert trace["status"] == "completed"
        assert len(trace["logs"]) == 4
        assert trace["duration_ms"] > 0

        # ë¡œê·¸ ìˆœì„œ ë° ë‚´ìš© í™•ì¸
        logs = trace["logs"]
        assert logs[0]["message"] == "Request started"
        assert logs[0]["endpoint"] == "/api/v1/places"
        assert logs[1]["message"] == "Database query executed"
        assert logs[2]["message"] == "Cache miss"
        assert logs[3]["message"] == "Request completed"

        print("âœ… ë¡œê·¸ ìƒê´€ê´€ê³„ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestCentralizedLogManagement:
    """ì¤‘ì•™ì§‘ì¤‘ì‹ ë¡œê·¸ ê´€ë¦¬ í…ŒìŠ¤íŠ¸"""

    def test_log_aggregation(self):
        """ë¡œê·¸ ì§‘ê³„ í…ŒìŠ¤íŠ¸"""
        # Given: ë‹¤ì¤‘ ì„œë¹„ìŠ¤ ë¡œê·¸ ìˆ˜ì§‘ê¸°
        log_collector = {"services": {}, "total_logs": 0, "log_buffer": []}

        def collect_log(service_name: str, log_entry: Dict[str, Any]):
            """ë¡œê·¸ ìˆ˜ì§‘"""
            if service_name not in log_collector["services"]:
                log_collector["services"][service_name] = {
                    "log_count": 0,
                    "last_log_time": None,
                    "error_count": 0,
                    "warning_count": 0,
                }

            service_stats = log_collector["services"][service_name]
            service_stats["log_count"] += 1
            service_stats["last_log_time"] = datetime.now()

            if log_entry.get("level") == "ERROR":
                service_stats["error_count"] += 1
            elif log_entry.get("level") == "WARNING":
                service_stats["warning_count"] += 1

            log_collector["total_logs"] += 1
            log_collector["log_buffer"].append({"service": service_name, **log_entry})

        def get_service_stats(service_name: str) -> Dict[str, Any]:
            """ì„œë¹„ìŠ¤ë³„ ë¡œê·¸ í†µê³„"""
            return log_collector["services"].get(service_name, {})

        # When: ë‹¤ì–‘í•œ ì„œë¹„ìŠ¤ì—ì„œ ë¡œê·¸ ìˆ˜ì§‘
        # API ì„œë¹„ìŠ¤ ë¡œê·¸
        collect_log(
            "hotly-api",
            {
                "level": "INFO",
                "message": "API request processed",
                "response_time_ms": 250,
            },
        )

        collect_log(
            "hotly-api",
            {
                "level": "ERROR",
                "message": "Database connection timeout",
                "error_code": "DB_TIMEOUT",
            },
        )

        # AI ì„œë¹„ìŠ¤ ë¡œê·¸
        collect_log(
            "hotly-ai",
            {
                "level": "INFO",
                "message": "Gemini API call successful",
                "tokens_used": 150,
            },
        )

        collect_log(
            "hotly-ai",
            {
                "level": "WARNING",
                "message": "AI response confidence low",
                "confidence": 0.65,
            },
        )

        # Cache ì„œë¹„ìŠ¤ ë¡œê·¸
        collect_log(
            "hotly-cache",
            {"level": "INFO", "message": "Cache hit", "cache_key": "places:user-456"},
        )

        # Then: ì§‘ê³„ ê²°ê³¼ ê²€ì¦
        assert log_collector["total_logs"] == 5
        assert len(log_collector["services"]) == 3

        api_stats = get_service_stats("hotly-api")
        assert api_stats["log_count"] == 2
        assert api_stats["error_count"] == 1
        assert api_stats["warning_count"] == 0

        ai_stats = get_service_stats("hotly-ai")
        assert ai_stats["log_count"] == 2
        assert ai_stats["error_count"] == 0
        assert ai_stats["warning_count"] == 1

        cache_stats = get_service_stats("hotly-cache")
        assert cache_stats["log_count"] == 1
        assert cache_stats["error_count"] == 0

        print("âœ… ë¡œê·¸ ì§‘ê³„ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_log_filtering_and_search(self):
        """ë¡œê·¸ í•„í„°ë§ ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        # Given: ë¡œê·¸ ê²€ìƒ‰ ì‹œìŠ¤í…œ
        log_database = []

        def add_log_entry(level: str, message: str, service: str, extra: Dict = None):
            """ë¡œê·¸ í•­ëª© ì¶”ê°€"""
            entry = {
                "id": len(log_database) + 1,
                "timestamp": datetime.now(),
                "level": level,
                "message": message,
                "service": service,
                **(extra or {}),
            }
            log_database.append(entry)
            return entry

        def search_logs(
            level: str = None,
            service: str = None,
            message_contains: str = None,
            time_from: datetime = None,
            time_to: datetime = None,
            limit: int = 100,
        ) -> List[Dict[str, Any]]:
            """ë¡œê·¸ ê²€ìƒ‰"""
            results = []

            for log in log_database:
                # ë ˆë²¨ í•„í„°
                if level and log["level"] != level:
                    continue

                # ì„œë¹„ìŠ¤ í•„í„°
                if service and log["service"] != service:
                    continue

                # ë©”ì‹œì§€ ê²€ìƒ‰
                if (
                    message_contains
                    and message_contains.lower() not in log["message"].lower()
                ):
                    continue

                # ì‹œê°„ ë²”ìœ„ í•„í„°
                if time_from and log["timestamp"] < time_from:
                    continue
                if time_to and log["timestamp"] > time_to:
                    continue

                results.append(log)

                if len(results) >= limit:
                    break

            return results

        # When: ë‹¤ì–‘í•œ ë¡œê·¸ ë°ì´í„° ì¶”ê°€
        add_log_entry(
            "INFO", "User logged in successfully", "hotly-api", {"user_id": "user-123"}
        )
        add_log_entry(
            "ERROR", "Database connection failed", "hotly-api", {"error": "timeout"}
        )
        add_log_entry(
            "INFO", "Cache hit for user data", "hotly-cache", {"cache_key": "user:123"}
        )
        add_log_entry(
            "WARNING", "AI response confidence low", "hotly-ai", {"confidence": 0.6}
        )
        add_log_entry("ERROR", "AI service unavailable", "hotly-ai", {"retry_count": 3})
        add_log_entry(
            "DEBUG", "Database query executed", "hotly-api", {"query_time_ms": 45}
        )

        # Then: ê²€ìƒ‰ ê²°ê³¼ ê²€ì¦

        # ì—ëŸ¬ ë¡œê·¸ë§Œ ê²€ìƒ‰
        error_logs = search_logs(level="ERROR")
        assert len(error_logs) == 2
        assert all(log["level"] == "ERROR" for log in error_logs)

        # API ì„œë¹„ìŠ¤ ë¡œê·¸ë§Œ ê²€ìƒ‰
        api_logs = search_logs(service="hotly-api")
        assert len(api_logs) == 3
        assert all(log["service"] == "hotly-api" for log in api_logs)

        # "user" í‚¤ì›Œë“œë¡œ ë©”ì‹œì§€ ê²€ìƒ‰
        user_logs = search_logs(message_contains="user")
        assert len(user_logs) >= 2  # "User logged in", "Cache hit for user data"

        # AI ì„œë¹„ìŠ¤ì˜ ì—ëŸ¬ ë¡œê·¸ë§Œ
        ai_errors = search_logs(level="ERROR", service="hotly-ai")
        assert len(ai_errors) == 1
        assert ai_errors[0]["message"] == "AI service unavailable"

        # ì œí•œëœ ê²°ê³¼ ìˆ˜
        limited_logs = search_logs(limit=3)
        assert len(limited_logs) == 3

        print("âœ… ë¡œê·¸ í•„í„°ë§ ë° ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_log_rotation_and_archiving(self):
        """ë¡œê·¸ ë¡œí…Œì´ì…˜ ë° ì•„ì¹´ì´ë¹™ í…ŒìŠ¤íŠ¸"""
        # Given: ë¡œê·¸ ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ
        MAX_LOG_SIZE = 1000  # ë¡œê·¸ íŒŒì¼ ìµœëŒ€ í¬ê¸° (ì‹œë®¬ë ˆì´ì…˜)
        MAX_FILES = 3  # ë³´ê´€í•  ë¡œê·¸ íŒŒì¼ ìˆ˜

        log_storage = {"current_logs": [], "archived_files": [], "current_size": 0}

        def add_log_entry(log_entry: Dict[str, Any]):
            """ë¡œê·¸ í•­ëª© ì¶”ê°€"""
            log_size = len(json.dumps(log_entry))  # ë¡œê·¸ í¬ê¸° ê³„ì‚°

            # ë¡œí…Œì´ì…˜ í•„ìš”í•œì§€ ì²´í¬
            if log_storage["current_size"] + log_size > MAX_LOG_SIZE:
                rotate_logs()

            log_storage["current_logs"].append(log_entry)
            log_storage["current_size"] += log_size

        def rotate_logs():
            """ë¡œê·¸ ë¡œí…Œì´ì…˜"""
            if log_storage["current_logs"]:
                # í˜„ì¬ ë¡œê·¸ë¥¼ ì•„ì¹´ì´ë¸Œë¡œ ì´ë™
                archived_file = {
                    "filename": f"app-{datetime.now().strftime('%Y%m%d_%H%M%S')}.log",
                    "logs": log_storage["current_logs"].copy(),
                    "size": log_storage["current_size"],
                    "archived_at": datetime.now(),
                }

                log_storage["archived_files"].append(archived_file)

                # ìµœëŒ€ íŒŒì¼ ìˆ˜ ì´ˆê³¼ ì‹œ ì˜¤ë˜ëœ íŒŒì¼ ì‚­ì œ
                while len(log_storage["archived_files"]) > MAX_FILES:
                    removed_file = log_storage["archived_files"].pop(0)
                    # ì‹¤ì œë¡œëŠ” íŒŒì¼ ì‹œìŠ¤í…œì—ì„œ ì‚­ì œ
                    print(f"Archived file deleted: {removed_file['filename']}")

                # í˜„ì¬ ë¡œê·¸ ì´ˆê¸°í™”
                log_storage["current_logs"] = []
                log_storage["current_size"] = 0

        def get_log_statistics() -> Dict[str, Any]:
            """ë¡œê·¸ í†µê³„"""
            total_logs = len(log_storage["current_logs"])
            total_size = log_storage["current_size"]

            for archived in log_storage["archived_files"]:
                total_logs += len(archived["logs"])
                total_size += archived["size"]

            return {
                "current_logs_count": len(log_storage["current_logs"]),
                "current_size": log_storage["current_size"],
                "archived_files_count": len(log_storage["archived_files"]),
                "total_logs": total_logs,
                "total_size": total_size,
            }

        # When: ë¡œí…Œì´ì…˜ í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•œ ì‹œë‚˜ë¦¬ì˜¤
        # ë¨¼ì € ë¡œí…Œì´ì…˜ì„ ê°•ì œë¡œ í•œ ë²ˆ ë°œìƒì‹œí‚´
        for i in range(10):
            add_log_entry(
                {
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "message": f"Large log entry {i}",
                    "service": "test-service",
                    "request_id": f"req-{i}",
                    "additional_data": "x" * 100,  # í° ë°ì´í„°ë¡œ ë¡œí…Œì´ì…˜ ìœ ë°œ
                }
            )

        # ì¶”ê°€ ë¡œê·¸ë¡œ ë” ë§ì€ ë¡œí…Œì´ì…˜ ìœ ë°œ
        for i in range(10, 15):
            add_log_entry(
                {
                    "timestamp": datetime.now().isoformat(),
                    "level": "INFO",
                    "message": f"Additional log {i}",
                    "service": "test-service",
                    "large_field": "y" * 200,  # ë” í° ë°ì´í„°
                }
            )

        # Then: ë¡œí…Œì´ì…˜ ì‹œìŠ¤í…œ ê¸°ë³¸ ë™ì‘ ê²€ì¦
        stats = get_log_statistics()

        # ê¸°ë³¸ ë¡œí…Œì´ì…˜ ê¸°ëŠ¥ í™•ì¸
        # ë¡œí…Œì´ì…˜ìœ¼ë¡œ ì¸í•´ ì¼ë¶€ ë¡œê·¸ê°€ ì‚­ì œë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ìµœì†Œí•œì˜ ë¡œê·¸ ë³´ì¡´ í™•ì¸
        assert stats["total_logs"] >= 5  # ìµœì†Œ 5ê°œ ë¡œê·¸ëŠ” ë³´ì¡´ë˜ì–´ì•¼ í•¨
        assert len(log_storage["archived_files"]) <= MAX_FILES  # ìµœëŒ€ íŒŒì¼ ìˆ˜ ì¤€ìˆ˜

        # ë¡œí…Œì´ì…˜ì´ ë°œìƒí–ˆë‹¤ë©´ ì•„ì¹´ì´ë¸Œ íŒŒì¼ êµ¬ì¡° í™•ì¸
        if log_storage["archived_files"]:
            archived_file = log_storage["archived_files"][-1]
            assert "filename" in archived_file
            assert "archived_at" in archived_file
            assert "logs" in archived_file
            assert isinstance(archived_file["logs"], list)

        # í˜„ì¬ ë¡œê·¸ ìƒíƒœ í™•ì¸
        if log_storage["current_logs"]:
            assert isinstance(log_storage["current_logs"], list)
            assert log_storage["current_size"] >= 0

        # í†µê³„ê°€ ì˜¬ë°”ë¥´ê²Œ ê³„ì‚°ë˜ëŠ”ì§€ í™•ì¸
        assert stats["total_size"] > 0
        assert stats["current_logs_count"] >= 0

        print("âœ… ë¡œê·¸ ë¡œí…Œì´ì…˜ ë° ì•„ì¹´ì´ë¹™ í…ŒìŠ¤íŠ¸ í†µê³¼")


def main():
    """ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ“ ë¡œê¹… ë° ì¤‘ì•™ì§‘ì¤‘ì‹ ê´€ë¦¬ ì‹œìŠ¤í…œ TDD í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 65)

    test_classes = [TestStructuredLogging(), TestCentralizedLogManagement()]

    total_passed = 0
    total_failed = 0

    for test_instance in test_classes:
        class_name = test_instance.__class__.__name__
        print(f"\nğŸ§ª {class_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("-" * 45)

        test_methods = [
            method for method in dir(test_instance) if method.startswith("test_")
        ]

        for method_name in test_methods:
            try:
                if hasattr(test_instance, "setup_method"):
                    test_instance.setup_method()

                test_method = getattr(test_instance, method_name)
                test_method()
                total_passed += 1
            except Exception as e:
                print(f"âŒ {method_name} ì‹¤íŒ¨: {e}")
                total_failed += 1

    print(f"\nğŸ“Š ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   âœ… í†µê³¼: {total_passed}")
    print(f"   âŒ ì‹¤íŒ¨: {total_failed}")
    print(f"   ğŸ“ˆ ì „ì²´: {total_passed + total_failed}")

    if total_failed == 0:
        print("ğŸ† ëª¨ë“  ë¡œê¹… ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print(f"âš ï¸ {total_failed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
