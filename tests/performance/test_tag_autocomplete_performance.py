#!/usr/bin/env python3
"""
Tag Autocomplete Performance Tests

Tests for validating 100ms response time requirement for tag autocomplete system.
Follows TDD approach for Task 1-2-4: íƒœê·¸ ê´€ë¦¬ ë° ìë™ì™„ì„± ì‹œìŠ¤í…œ (100ms)
"""

import time
from unittest.mock import Mock
from uuid import uuid4

from app.services.utils.tag_service import TagService
from app.utils.tag_normalizer import TagNormalizer


class TestTagAutocompletePerformance:
    """
    Performance tests for tag autocomplete system.

    Validates that autocomplete responses meet 100ms requirement
    under various load conditions.
    """

    def setup_method(self):
        """Setup test data and mock database."""
        self.mock_db = Mock()
        self.tag_service = TagService(self.mock_db)

        # Performance threshold
        self.autocomplete_threshold_ms = 100  # 100ms requirement

        # Setup mock data for performance testing
        self._setup_mock_data()

    def _setup_mock_data(self):
        """Setup mock database responses for consistent performance testing."""
        # Mock common Korean tags for autocomplete testing
        self.common_korean_tags = [
            ("ì¹´í˜", 25),
            ("ë§›ì§‘", 22),
            ("í•œì‹", 18),
            ("ì¹˜í‚¨", 15),
            ("ì»¤í”¼", 14),
            ("ë¸ŒëŸ°ì¹˜", 12),
            ("ë””ì €íŠ¸", 11),
            ("ë¶„ì‹", 10),
            ("ë…¸ë˜ë°©", 9),
            ("ìˆ ì§‘", 8),
            ("ë°±í™”ì ", 7),
            ("ì‡¼í•‘", 6),
            ("ê´€ê´‘", 5),
            ("ë¬¸í™”", 4),
            ("ì „í†µ", 3),
            ("ì—­ì‚¬", 2),
        ]

        # Setup mock query results
        def mock_query_side_effect(*args, **kwargs):
            mock_query = Mock()

            # Mock the chained query methods
            mock_query.filter.return_value = mock_query
            mock_query.group_by.return_value = mock_query
            mock_query.order_by.return_value = mock_query
            mock_query.limit.return_value = mock_query

            # Mock results based on tag data
            mock_results = []
            for tag, count in self.common_korean_tags:
                mock_result = Mock()
                mock_result.tag = tag
                mock_result.count = count
                mock_results.append(mock_result)

            mock_query.all.return_value = mock_results
            return mock_query

        self.mock_db.query.side_effect = mock_query_side_effect

    async def measure_autocomplete_time(self, query: str, user_id: str = None) -> tuple:
        """Measure tag autocomplete response time."""
        if user_id is None:
            user_id = uuid4()

        start_time = time.perf_counter()

        suggestions = self.tag_service.get_tag_suggestions(
            query=query, user_id=user_id, limit=10
        )

        end_time = time.perf_counter()
        execution_time_ms = (end_time - start_time) * 1000

        return execution_time_ms, suggestions

    def test_autocomplete_performance_commonQueries_under100ms(self):
        """
        Test: Common autocomplete queries complete under 100ms

        RED: This test validates current performance meets requirements
        """
        # Given: Common Korean search queries
        test_queries = [
            "ì¹´",  # Single character
            "ì¹´í˜",  # Common complete word
            "ë§›",  # Partial word
            "í•œì‹ë‹¹",  # Complex query
            "ì»¤í”¼ìˆ",  # Another common term
            "ì¹˜í‚¨ì§‘",  # Korean-specific business
            "ë…¸ë˜",  # Entertainment prefix
            "ì‡¼í•‘ëª°",  # Shopping related
        ]

        performance_results = []

        # When: Execute autocomplete for each query
        for query in test_queries:
            execution_time_ms, suggestions = self.measure_autocomplete_time(query)

            performance_results.append(
                {
                    "query": query,
                    "time_ms": execution_time_ms,
                    "suggestion_count": len(suggestions),
                    "under_threshold": execution_time_ms
                    < self.autocomplete_threshold_ms,
                }
            )

            # Then: Should complete under 100ms
            assert (
                execution_time_ms < self.autocomplete_threshold_ms
            ), f"Autocomplete for '{query}' took {execution_time_ms:.1f}ms, expected < {self.autocomplete_threshold_ms}ms"

        # Print performance summary
        print("\nâš¡ Tag Autocomplete Performance Results:")
        print(f"   Threshold: {self.autocomplete_threshold_ms}ms")

        total_time = sum(result["time_ms"] for result in performance_results)
        avg_time = total_time / len(performance_results)

        print(f"   Average response time: {avg_time:.1f}ms")
        print(f"   Total queries tested: {len(performance_results)}")

        for result in performance_results:
            status = "âœ…" if result["under_threshold"] else "âŒ"
            print(
                f"   {status} '{result['query']}': {result['time_ms']:.1f}ms "
                f"({result['suggestion_count']} suggestions)"
            )

        # All queries should meet threshold
        failed_queries = [r for r in performance_results if not r["under_threshold"]]
        assert (
            len(failed_queries) == 0
        ), f"{len(failed_queries)} queries exceeded threshold"

    def test_autocomplete_scalability_increasingDataSize_maintainPerformance(self):
        """
        Test: Autocomplete maintains performance as data size increases

        Simulates performance under different user tag collection sizes
        """
        # Given: Different user data sizes
        data_sizes = [10, 50, 100, 500, 1000]  # Number of user tags
        query = "ì¹´í˜"

        performance_by_size = []

        for size in data_sizes:
            # Mock larger dataset for this user
            large_dataset = self.common_korean_tags * (
                size // len(self.common_korean_tags) + 1
            )
            large_dataset = large_dataset[:size]

            # Update mock to return larger dataset
            def mock_large_query(*args, **kwargs):
                mock_query = Mock()
                mock_query.filter.return_value = mock_query
                mock_query.group_by.return_value = mock_query
                mock_query.order_by.return_value = mock_query
                mock_query.limit.return_value = mock_query

                mock_results = []
                for tag, count in large_dataset:
                    mock_result = Mock()
                    mock_result.tag = tag
                    mock_result.count = count
                    mock_results.append(mock_result)

                mock_query.all.return_value = mock_results
                return mock_query

            self.mock_db.query.side_effect = mock_large_query

            # When: Measure performance
            execution_time_ms, suggestions = self.measure_autocomplete_time(query)

            performance_by_size.append(
                {
                    "data_size": size,
                    "time_ms": execution_time_ms,
                    "suggestions": len(suggestions),
                }
            )

            # Then: Should still meet performance threshold
            assert (
                execution_time_ms < self.autocomplete_threshold_ms
            ), f"Performance degraded with {size} tags: {execution_time_ms:.1f}ms"

        print("\nğŸ“ˆ Scalability Test Results:")
        for result in performance_by_size:
            print(f"   {result['data_size']} tags: {result['time_ms']:.1f}ms")

        # Performance should not degrade significantly
        min_time = min(r["time_ms"] for r in performance_by_size)
        max_time = max(r["time_ms"] for r in performance_by_size)
        performance_ratio = max_time / min_time

        assert (
            performance_ratio < 3.0
        ), f"Performance degraded too much: {performance_ratio:.1f}x slower"

    def test_korean_text_normalization_performance_complexInputs_efficientProcessing(
        self,
    ):
        """
        Test: Korean text normalization performs efficiently

        Critical for Korean autocomplete performance
        """
        # Given: Complex Korean normalization cases
        korean_test_cases = [
            "í•œêµ­ ì „í†µ ìŒì‹ì ",  # Spaces and traditional terms
            "ìŠ¤íƒ€ë²…ìŠ¤@í™ëŒ€ì…êµ¬ì—­",  # Mixed Korean/English with symbols
            "ì¹˜í‚¨&ë§¥ì£¼ ì „ë¬¸ì !!!",  # Special characters
            "íŒŒë¦¬ë°”ê²Œëœ¨ (ë² ì´ì»¤ë¦¬)",  # Parentheses and brand names
            "24ì‹œ í¸ì˜ì /ë§ˆíŠ¸",  # Numbers and slashes
        ]

        normalizer = TagNormalizer()

        for test_input in korean_test_cases:
            # When: Normalize Korean text
            start_time = time.perf_counter()

            normalized = normalizer.normalize_tag(test_input)

            end_time = time.perf_counter()
            normalization_time_ms = (end_time - start_time) * 1000

            # Then: Should normalize quickly (well under 100ms)
            assert (
                normalization_time_ms < 10
            ), f"Korean normalization too slow for '{test_input}': {normalization_time_ms:.2f}ms"

            print(
                f"   âœ… '{test_input}' â†’ '{normalized}' ({normalization_time_ms:.2f}ms)"
            )

    def test_concurrent_autocomplete_requests_multipleUsers_noPerformanceDegradation(
        self,
    ):
        """
        Test: Concurrent autocomplete requests maintain performance

        Simulates multiple users using autocomplete simultaneously
        """
        import concurrent.futures

        # Given: Multiple concurrent requests
        num_concurrent_requests = 10
        test_query = "ë§›ì§‘"

        def single_autocomplete_request():
            user_id = uuid4()
            execution_time_ms, suggestions = self.measure_autocomplete_time(
                test_query, user_id
            )
            return execution_time_ms

        # When: Execute concurrent requests
        start_time = time.perf_counter()

        with concurrent.futures.ThreadPoolExecutor(
            max_workers=num_concurrent_requests
        ) as executor:
            futures = [
                executor.submit(single_autocomplete_request)
                for _ in range(num_concurrent_requests)
            ]

            execution_times = [
                future.result() for future in concurrent.futures.as_completed(futures)
            ]

        total_time = time.perf_counter() - start_time

        # Then: All requests should meet performance threshold
        max_time = max(execution_times)
        avg_time = sum(execution_times) / len(execution_times)

        print("\nğŸ”„ Concurrent Request Results:")
        print(f"   Concurrent requests: {num_concurrent_requests}")
        print(f"   Total time: {total_time * 1000:.1f}ms")
        print(f"   Average per request: {avg_time:.1f}ms")
        print(f"   Slowest request: {max_time:.1f}ms")

        assert (
            max_time < self.autocomplete_threshold_ms
        ), f"Slowest concurrent request took {max_time:.1f}ms, expected < {self.autocomplete_threshold_ms}ms"

        # Total time should be reasonable (not just sequential)
        expected_sequential_time = num_concurrent_requests * avg_time
        concurrency_improvement = expected_sequential_time / (total_time * 1000)

        assert (
            concurrency_improvement > 2.0
        ), f"Poor concurrency: only {concurrency_improvement:.1f}x improvement"

    def test_edge_case_performance_emptyAndSpecialInputs_robustHandling(self):
        """
        Test: Edge cases are handled efficiently without performance impact

        Tests autocomplete with problematic inputs
        """
        # Given: Edge case inputs
        edge_cases = [
            "",  # Empty string
            " ",  # Whitespace only
            "a",  # Single character
            "ã„±ã„´ã„·",  # Korean consonants only
            "123",  # Numbers only
            "!@#$%",  # Special characters only
            "a" * 100,  # Very long input
        ]

        for edge_input in edge_cases:
            # When: Process edge case
            execution_time_ms, suggestions = self.measure_autocomplete_time(edge_input)

            # Then: Should handle gracefully and quickly
            assert (
                execution_time_ms < 50
            ), f"Edge case '{edge_input}' took too long: {execution_time_ms:.1f}ms"

            # Should return appropriate response (empty or fallback)
            assert isinstance(
                suggestions, list
            ), f"Invalid response type for '{edge_input}'"

            print(f"   âœ… Edge case '{edge_input}' handled in {execution_time_ms:.1f}ms")

    def test_caching_effectiveness_repeatedQueries_improvedPerformance(self):
        """
        Test: Repeated queries benefit from caching (if implemented)

        Measures performance improvement on repeated autocomplete requests
        """
        # Given: Same query repeated multiple times
        query = "í•œì‹"
        num_repeats = 5

        execution_times = []

        # When: Execute same query multiple times
        for i in range(num_repeats):
            execution_time_ms, suggestions = self.measure_autocomplete_time(query)
            execution_times.append(execution_time_ms)

        # Then: Later queries should be as fast or faster
        first_time = execution_times[0]
        avg_later_time = sum(execution_times[1:]) / len(execution_times[1:])

        print("\nğŸ”„ Caching Test Results:")
        print(f"   First query: {first_time:.1f}ms")
        print(f"   Average later queries: {avg_later_time:.1f}ms")

        # All queries should still meet threshold regardless
        max_time = max(execution_times)
        assert (
            max_time < self.autocomplete_threshold_ms
        ), f"Repeated query exceeded threshold: {max_time:.1f}ms"
