"""
ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸

ìºì‹œ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, ìµœì í™”, ë©”íŠ¸ë¦­ ìˆ˜ì§‘ì„ ìœ„í•œ TDD í…ŒìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""
import statistics

# import pytest
import time
from dataclasses import dataclass
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional


@dataclass
class CacheMetrics:
    """ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""

    hit_count: int = 0
    miss_count: int = 0
    total_requests: int = 0
    response_times: List[float] = None
    memory_usage: Dict[str, int] = None
    cache_sizes: Dict[str, int] = None

    def __post_init__(self):
        if self.response_times is None:
            self.response_times = []
        if self.memory_usage is None:
            self.memory_usage = {"l1": 0, "l2": 0, "l3": 0}
        if self.cache_sizes is None:
            self.cache_sizes = {"l1": 0, "l2": 0, "l3": 0}

    @property
    def hit_rate(self) -> float:
        """ìºì‹œ ì ì¤‘ë¥ """
        if self.total_requests == 0:
            return 0.0
        return self.hit_count / self.total_requests

    @property
    def miss_rate(self) -> float:
        """ìºì‹œ ë¯¸ìŠ¤ìœ¨"""
        return 1.0 - self.hit_rate

    @property
    def avg_response_time(self) -> float:
        """í‰ê·  ì‘ë‹µ ì‹œê°„"""
        if not self.response_times:
            return 0.0
        return statistics.mean(self.response_times)

    @property
    def p95_response_time(self) -> float:
        """95í¼ì„¼íƒ€ì¼ ì‘ë‹µ ì‹œê°„"""
        if not self.response_times or len(self.response_times) < 2:
            return 0.0
        sorted_times = sorted(self.response_times)
        index = int(0.95 * len(sorted_times))
        return sorted_times[min(index, len(sorted_times) - 1)]


class TestCachePerformanceMetrics:
    """ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ í…ŒìŠ¤íŠ¸"""

    def setup_method(self):
        """í…ŒìŠ¤íŠ¸ ì„¤ì •"""
        self.metrics = CacheMetrics()

    def test_cache_hit_rate_calculation(self):
        """ìºì‹œ ì ì¤‘ë¥  ê³„ì‚° í…ŒìŠ¤íŠ¸"""
        # Given: ìºì‹œ ìš”ì²­ í†µê³„
        self.metrics.hit_count = 70
        self.metrics.miss_count = 30
        self.metrics.total_requests = 100

        # When: ì ì¤‘ë¥  ê³„ì‚°
        hit_rate = self.metrics.hit_rate
        miss_rate = self.metrics.miss_rate

        # Then: ì˜¬ë°”ë¥¸ ì ì¤‘ë¥  ê³„ì‚°
        assert hit_rate == 0.7
        assert abs(miss_rate - 0.3) < 0.001  # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©
        assert abs(hit_rate + miss_rate - 1.0) < 0.001  # ë¶€ë™ì†Œìˆ˜ì  ì˜¤ì°¨ í—ˆìš©

        print("âœ… ìºì‹œ ì ì¤‘ë¥  ê³„ì‚° í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_response_time_statistics(self):
        """ì‘ë‹µ ì‹œê°„ í†µê³„ í…ŒìŠ¤íŠ¸"""
        # Given: ì‘ë‹µ ì‹œê°„ ë°ì´í„°
        response_times = [0.1, 0.2, 0.15, 0.3, 0.25, 0.4, 0.2, 0.1, 0.35, 0.5]
        self.metrics.response_times = response_times

        # When: í†µê³„ ê³„ì‚°
        avg_time = self.metrics.avg_response_time

        # Then: ì˜¬ë°”ë¥¸ í†µê³„ ê³„ì‚°
        expected_avg = sum(response_times) / len(response_times)
        assert abs(avg_time - expected_avg) < 0.01
        assert avg_time > 0

        print("âœ… ì‘ë‹µ ì‹œê°„ í†µê³„ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_memory_usage_tracking(self):
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        # Given: ê³„ì¸µë³„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        self.metrics.memory_usage = {
            "l1": 50 * 1024 * 1024,  # 50MB
            "l2": 200 * 1024 * 1024,  # 200MB
            "l3": 500 * 1024 * 1024,  # 500MB
        }

        # When: ì´ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°
        total_memory = sum(self.metrics.memory_usage.values())
        memory_mb = {k: v / (1024 * 1024) for k, v in self.metrics.memory_usage.items()}

        # Then: ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
        assert memory_mb["l1"] == 50
        assert memory_mb["l2"] == 200
        assert memory_mb["l3"] == 500
        assert total_memory == 750 * 1024 * 1024  # 750MB

        print("âœ… ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestPerformanceMonitoring:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""

    def test_performance_monitoring_collection(self):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
        # Given: ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì‹œìŠ¤í…œ
        performance_data = []

        def collect_performance_metrics() -> Dict[str, Any]:
            """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
            timestamp = datetime.now().isoformat()
            metrics = {
                "timestamp": timestamp,
                "cache_hit_rate": 0.75,
                "avg_response_time": 0.23,
                "memory_usage_mb": 156,
                "active_connections": 42,
                "cpu_usage_percent": 15.5,
                "requests_per_second": 120,
            }
            performance_data.append(metrics)
            return metrics

        def get_performance_trends(minutes: int = 5) -> Dict[str, List[float]]:
            """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
            cutoff_time = datetime.now() - timedelta(minutes=minutes)
            recent_data = [
                data
                for data in performance_data
                if datetime.fromisoformat(data["timestamp"]) >= cutoff_time
            ]

            if not recent_data:
                return {}

            return {
                "hit_rates": [d["cache_hit_rate"] for d in recent_data],
                "response_times": [d["avg_response_time"] for d in recent_data],
                "memory_usage": [d["memory_usage_mb"] for d in recent_data],
            }

        # When: ì—¬ëŸ¬ ì‹œì ì˜ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘
        for i in range(5):
            collect_performance_metrics()
            time.sleep(0.01)  # ì‹œê°„ ê°„ê²©

        trends = get_performance_trends(10)

        # Then: ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ë° íŠ¸ë Œë“œ ë¶„ì„
        assert len(performance_data) == 5
        assert all("timestamp" in data for data in performance_data)
        assert all("cache_hit_rate" in data for data in performance_data)

        assert "hit_rates" in trends
        assert "response_times" in trends
        assert len(trends["hit_rates"]) == 5

        print("âœ… ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_performance_alerting_system(self):
        """ì„±ëŠ¥ ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        # Given: ì„±ëŠ¥ ì•Œë¦¼ ì‹œìŠ¤í…œ
        alerts = []
        thresholds = {
            "max_response_time": 1.0,  # 1ì´ˆ
            "min_hit_rate": 0.6,  # 60%
            "max_memory_usage": 200,  # 200MB
            "max_cpu_usage": 80,  # 80%
        }

        def check_performance_alerts(metrics: Dict[str, Any]):
            """ì„±ëŠ¥ ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼"""
            current_alerts = []

            if metrics["avg_response_time"] > thresholds["max_response_time"]:
                current_alerts.append(
                    {
                        "type": "high_response_time",
                        "value": metrics["avg_response_time"],
                        "threshold": thresholds["max_response_time"],
                        "severity": "warning",
                    }
                )

            if metrics["cache_hit_rate"] < thresholds["min_hit_rate"]:
                current_alerts.append(
                    {
                        "type": "low_hit_rate",
                        "value": metrics["cache_hit_rate"],
                        "threshold": thresholds["min_hit_rate"],
                        "severity": "warning",
                    }
                )

            if metrics["memory_usage_mb"] > thresholds["max_memory_usage"]:
                current_alerts.append(
                    {
                        "type": "high_memory_usage",
                        "value": metrics["memory_usage_mb"],
                        "threshold": thresholds["max_memory_usage"],
                        "severity": "critical",
                    }
                )

            alerts.extend(current_alerts)
            return current_alerts

        # When: ì„ê³„ê°’ì„ ì´ˆê³¼í•˜ëŠ” ë©”íŠ¸ë¦­
        problematic_metrics = {
            "cache_hit_rate": 0.45,  # ì„ê³„ê°’ ë¯¸ë‹¬
            "avg_response_time": 1.5,  # ì„ê³„ê°’ ì´ˆê³¼
            "memory_usage_mb": 250,  # ì„ê³„ê°’ ì´ˆê³¼
            "cpu_usage_percent": 45,
        }

        normal_metrics = {
            "cache_hit_rate": 0.75,
            "avg_response_time": 0.3,
            "memory_usage_mb": 150,
            "cpu_usage_percent": 25,
        }

        problem_alerts = check_performance_alerts(problematic_metrics)
        normal_alerts = check_performance_alerts(normal_metrics)

        # Then: ì ì ˆí•œ ì•Œë¦¼ ìƒì„±
        assert len(problem_alerts) == 3  # 3ê°œ ì„ê³„ê°’ ì´ˆê³¼
        assert len(normal_alerts) == 0  # ì •ìƒ ìƒíƒœ

        alert_types = [alert["type"] for alert in problem_alerts]
        assert "low_hit_rate" in alert_types
        assert "high_response_time" in alert_types
        assert "high_memory_usage" in alert_types

        # ì‹¬ê°ë„ ì²´í¬
        memory_alert = next(
            a for a in problem_alerts if a["type"] == "high_memory_usage"
        )
        assert memory_alert["severity"] == "critical"

        print("âœ… ì„±ëŠ¥ ì•Œë¦¼ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestCacheOptimization:
    """ìºì‹œ ìµœì í™” í…ŒìŠ¤íŠ¸"""

    def test_cache_warming_strategy(self):
        """ìºì‹œ ì›Œë° ì „ëµ í…ŒìŠ¤íŠ¸"""
        # Given: ìºì‹œ ì›Œë° ì‹œìŠ¤í…œ
        cache = {}
        popular_keys = ["place:123", "place:456", "place:789"]

        def warm_cache_with_popular_data():
            """ì¸ê¸° ë°ì´í„°ë¡œ ìºì‹œ ì›Œë°"""
            warmed_count = 0

            for key in popular_keys:
                # ì¸ê¸° ë°ì´í„°ë¥¼ ë¯¸ë¦¬ ìºì‹œì— ë¡œë“œ
                cache_data = {
                    "key": key,
                    "data": f"popular_data_for_{key}",
                    "warmed_at": datetime.now().isoformat(),
                    "is_warmed": True,
                }
                cache[key] = cache_data
                warmed_count += 1

            return warmed_count

        def get_cache_warming_status() -> Dict[str, Any]:
            """ìºì‹œ ì›Œë° ìƒíƒœ ì¡°íšŒ"""
            warmed_keys = [k for k, v in cache.items() if v.get("is_warmed", False)]
            return {
                "total_warmed": len(warmed_keys),
                "warmed_keys": warmed_keys,
                "warming_rate": len(warmed_keys) / len(popular_keys)
                if popular_keys
                else 0,
            }

        # When: ìºì‹œ ì›Œë° ì‹¤í–‰
        warmed_count = warm_cache_with_popular_data()
        warming_status = get_cache_warming_status()

        # Then: ìºì‹œ ì›Œë° ì„±ê³µ
        assert warmed_count == 3
        assert warming_status["total_warmed"] == 3
        assert warming_status["warming_rate"] == 1.0

        # ì›Œë°ëœ ë°ì´í„° ê²€ì¦
        for key in popular_keys:
            assert key in cache
            assert cache[key]["is_warmed"] is True

        print("âœ… ìºì‹œ ì›Œë° ì „ëµ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_cache_preloading_optimization(self):
        """ìºì‹œ í”„ë¦¬ë¡œë”© ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # Given: ì‚¬ìš©ì í–‰ë™ ê¸°ë°˜ í”„ë¦¬ë¡œë”© ì‹œìŠ¤í…œ
        user_access_patterns = {
            "user_123": ["place:1", "place:2", "place:3"],
            "user_456": ["place:2", "place:4", "place:5"],
        }
        cache = {}
        preload_queue = []

        def predict_next_access(user_id: str, current_key: str) -> List[str]:
            """ì‚¬ìš©ì íŒ¨í„´ ê¸°ë°˜ ë‹¤ìŒ ì ‘ê·¼ ì˜ˆì¸¡"""
            if user_id not in user_access_patterns:
                return []

            user_pattern = user_access_patterns[user_id]
            try:
                current_index = user_pattern.index(current_key)
                # ë‹¤ìŒ 2ê°œ í•­ëª© ì˜ˆì¸¡
                next_items = user_pattern[current_index + 1 : current_index + 3]
                return next_items
            except ValueError:
                return user_pattern[:2]  # íŒ¨í„´ ì‹œì‘ë¶€ë¶„ ë°˜í™˜

        def preload_cache(user_id: str, predicted_keys: List[str]):
            """ì˜ˆì¸¡ëœ í‚¤ë“¤ì„ ìºì‹œì— í”„ë¦¬ë¡œë”©"""
            for key in predicted_keys:
                if key not in cache:
                    cache[key] = {
                        "data": f"preloaded_data_for_{key}",
                        "preloaded_for_user": user_id,
                        "preloaded_at": datetime.now().isoformat(),
                        "is_preloaded": True,
                    }
                    preload_queue.append(key)

            return len(predicted_keys)

        # When: ì‚¬ìš©ì ì ‘ê·¼ íŒ¨í„´ ê¸°ë°˜ í”„ë¦¬ë¡œë”©
        user_id = "user_123"
        current_access = "place:1"

        predicted_keys = predict_next_access(user_id, current_access)
        preloaded_count = preload_cache(user_id, predicted_keys)

        # Then: ì˜ˆì¸¡ ê¸°ë°˜ í”„ë¦¬ë¡œë”© ì„±ê³µ
        assert predicted_keys == ["place:2", "place:3"]
        assert preloaded_count == 2
        assert len(preload_queue) == 2

        # í”„ë¦¬ë¡œë”©ëœ ë°ì´í„° ê²€ì¦
        for key in predicted_keys:
            assert key in cache
            assert cache[key]["is_preloaded"] is True
            assert cache[key]["preloaded_for_user"] == user_id

        print("âœ… ìºì‹œ í”„ë¦¬ë¡œë”© ìµœì í™” í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_cache_compression_optimization(self):
        """ìºì‹œ ì••ì¶• ìµœì í™” í…ŒìŠ¤íŠ¸"""
        # Given: ìºì‹œ ì••ì¶• ì‹œìŠ¤í…œ
        import base64
        import gzip
        import json

        cache_storage = {}
        compression_stats = {"original_size": 0, "compressed_size": 0}

        def compress_cache_data(data: Any) -> str:
            """ìºì‹œ ë°ì´í„° ì••ì¶•"""
            json_str = json.dumps(data)
            json_bytes = json_str.encode("utf-8")

            compression_stats["original_size"] += len(json_bytes)

            # gzip ì••ì¶•
            compressed = gzip.compress(json_bytes)
            compression_stats["compressed_size"] += len(compressed)

            # base64 ì¸ì½”ë”©í•˜ì—¬ ë¬¸ìì—´ë¡œ ì €ì¥
            return base64.b64encode(compressed).decode("utf-8")

        def decompress_cache_data(compressed_data: str) -> Any:
            """ìºì‹œ ë°ì´í„° ì••ì¶• í•´ì œ"""
            compressed_bytes = base64.b64decode(compressed_data.encode("utf-8"))
            decompressed_bytes = gzip.decompress(compressed_bytes)
            json_str = decompressed_bytes.decode("utf-8")
            return json.loads(json_str)

        def store_compressed_cache(key: str, data: Any):
            """ì••ì¶•ëœ í˜•íƒœë¡œ ìºì‹œ ì €ì¥"""
            compressed = compress_cache_data(data)
            cache_storage[key] = {
                "compressed_data": compressed,
                "is_compressed": True,
                "stored_at": datetime.now().isoformat(),
            }

        def get_compressed_cache(key: str) -> Optional[Any]:
            """ì••ì¶•ëœ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
            if key not in cache_storage:
                return None

            cache_entry = cache_storage[key]
            if cache_entry["is_compressed"]:
                return decompress_cache_data(cache_entry["compressed_data"])
            return cache_entry["data"]

        # When: í° ë°ì´í„°ë¥¼ ì••ì¶•í•˜ì—¬ ìºì‹œ ì €ì¥
        large_data = {
            "places": [
                {"id": i, "name": f"Place {i}", "description": "A" * 100}
                for i in range(50)  # í° ë°ì´í„°ì…‹
            ],
            "metadata": {"total": 50, "generated_at": datetime.now().isoformat()},
        }

        store_compressed_cache("large_dataset", large_data)
        retrieved_data = get_compressed_cache("large_dataset")

        # Then: ì••ì¶•/í•´ì œ ì„±ê³µ ë° ì••ì¶•ë¥  í™•ì¸
        assert retrieved_data == large_data
        assert compression_stats["compressed_size"] < compression_stats["original_size"]

        compression_ratio = (
            compression_stats["compressed_size"] / compression_stats["original_size"]
        )
        assert compression_ratio < 0.8  # ìµœì†Œ 20% ì••ì¶•

        print(f"âœ… ìºì‹œ ì••ì¶• ìµœì í™” í…ŒìŠ¤íŠ¸ í†µê³¼ (ì••ì¶•ë¥ : {compression_ratio:.2%})")


class TestCachePerformanceBenchmark:
    """ìºì‹œ ì„±ëŠ¥ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""

    def test_cache_throughput_benchmark(self):
        """ìºì‹œ ì²˜ë¦¬ëŸ‰ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        # Given: ìºì‹œ ì²˜ë¦¬ëŸ‰ ë²¤ì¹˜ë§ˆí¬ ì‹œìŠ¤í…œ
        cache = {}
        benchmark_results = []

        def benchmark_cache_operations(operation_count: int = 1000):
            """ìºì‹œ ì—°ì‚° ë²¤ì¹˜ë§ˆí¬"""
            # SET ì—°ì‚° ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            for i in range(operation_count):
                cache[f"bench_key_{i}"] = f"bench_value_{i}"
            set_duration = time.time() - start_time

            # GET ì—°ì‚° ë²¤ì¹˜ë§ˆí¬
            start_time = time.time()
            for i in range(operation_count):
                _ = cache.get(f"bench_key_{i}")
            get_duration = time.time() - start_time

            return {
                "operation_count": operation_count,
                "set_duration": set_duration,
                "get_duration": get_duration,
                "set_ops_per_sec": operation_count / set_duration,
                "get_ops_per_sec": operation_count / get_duration,
            }

        # When: ë²¤ì¹˜ë§ˆí¬ ì‹¤í–‰
        results = benchmark_cache_operations(1000)
        benchmark_results.append(results)

        # Then: ì„±ëŠ¥ ê¸°ì¤€ ì¶©ì¡±
        assert results["set_ops_per_sec"] > 10000  # ì´ˆë‹¹ 10,000íšŒ ì´ìƒ
        assert results["get_ops_per_sec"] > 50000  # ì´ˆë‹¹ 50,000íšŒ ì´ìƒ
        assert results["set_duration"] < 0.5  # 0.5ì´ˆ ì´ë‚´
        assert results["get_duration"] < 0.1  # 0.1ì´ˆ ì´ë‚´

        print(f"âœ… ìºì‹œ ì²˜ë¦¬ëŸ‰ ë²¤ì¹˜ë§ˆí¬ í†µê³¼")
        print(f"   SET: {results['set_ops_per_sec']:.0f} ops/sec")
        print(f"   GET: {results['get_ops_per_sec']:.0f} ops/sec")

    def test_cache_latency_benchmark(self):
        """ìºì‹œ ì§€ì—°ì‹œê°„ ë²¤ì¹˜ë§ˆí¬ í…ŒìŠ¤íŠ¸"""
        # Given: ì§€ì—°ì‹œê°„ ì¸¡ì • ì‹œìŠ¤í…œ
        cache = {"test_key": "test_value"}

        def measure_cache_latency(iterations: int = 100):
            """ìºì‹œ ì ‘ê·¼ ì§€ì—°ì‹œê°„ ì¸¡ì •"""
            latencies = []

            for _ in range(iterations):
                start_time = time.perf_counter()
                _ = cache.get("test_key")
                end_time = time.perf_counter()

                latency_ms = (end_time - start_time) * 1000  # ë°€ë¦¬ì´ˆ ë³€í™˜
                latencies.append(latency_ms)

            return {
                "min_latency": min(latencies),
                "max_latency": max(latencies),
                "avg_latency": statistics.mean(latencies),
                "p95_latency": sorted(latencies)[int(0.95 * len(latencies))]
                if len(latencies) > 1
                else latencies[0]
                if latencies
                else 0,
                "measurements": len(latencies),
            }

        # When: ì§€ì—°ì‹œê°„ ì¸¡ì •
        latency_stats = measure_cache_latency(100)

        # Then: ì§€ì—°ì‹œê°„ ê¸°ì¤€ ì¶©ì¡±
        assert latency_stats["avg_latency"] < 1.0  # í‰ê·  1ms ë¯¸ë§Œ
        assert latency_stats["p95_latency"] < 2.0  # P95 2ms ë¯¸ë§Œ
        assert latency_stats["max_latency"] < 10.0  # ìµœëŒ€ 10ms ë¯¸ë§Œ

        print(f"âœ… ìºì‹œ ì§€ì—°ì‹œê°„ ë²¤ì¹˜ë§ˆí¬ í†µê³¼")
        print(f"   í‰ê· : {latency_stats['avg_latency']:.3f}ms")
        print(f"   P95: {latency_stats['p95_latency']:.3f}ms")
        print(f"   ìµœëŒ€: {latency_stats['max_latency']:.3f}ms")


def main():
    """ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸš€ ìºì‹œ ì„±ëŠ¥ ë° ìµœì í™” TDD í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    test_classes = [
        TestCachePerformanceMetrics(),
        TestPerformanceMonitoring(),
        TestCacheOptimization(),
        TestCachePerformanceBenchmark(),
    ]

    total_passed = 0
    total_failed = 0

    for test_instance in test_classes:
        class_name = test_instance.__class__.__name__
        print(f"\nğŸ“Š {class_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("-" * 40)

        test_methods = [
            method for method in dir(test_instance) if method.startswith("test_")
        ]

        for method_name in test_methods:
            try:
                if hasattr(test_instance, "setup_method"):
                    test_instance.setup_method()

                test_method = getattr(test_instance, method_name)
                test_method()
                total_passed += 1
            except Exception as e:
                print(f"âŒ {method_name} ì‹¤íŒ¨: {e}")
                total_failed += 1

    print(f"\nğŸ“ˆ ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   âœ… í†µê³¼: {total_passed}")
    print(f"   âŒ ì‹¤íŒ¨: {total_failed}")
    print(f"   ğŸ“Š ì „ì²´: {total_passed + total_failed}")

    if total_failed == 0:
        print("ğŸ† ëª¨ë“  ìºì‹œ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print(f"âš ï¸ {total_failed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
