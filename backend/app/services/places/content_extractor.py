"""Content extraction service for SNS link analysis."""

import asyncio
import re
import time
from urllib.parse import urlparse

from app.exceptions.external import ContentExtractionError, UnsupportedPlatformError
from app.schemas.content import ContentMetadata, ExtractedContent, PlatformType


class ContentExtractor:
    """Extract content and metadata from SNS links."""

    def __init__(self) -> None:
        """Initialize content extractor."""
        self.timeout = 30  # 30 seconds timeout

    def _detect_platform(self, url: str) -> PlatformType:
        """Detect platform from URL."""
        parsed = urlparse(url)
        domain = parsed.netloc.lower()

        if "instagram.com" in domain:
            return PlatformType.INSTAGRAM
        elif "blog.naver.com" in domain:
            return PlatformType.NAVER_BLOG
        elif "youtube.com" in domain or "youtu.be" in domain:
            return PlatformType.YOUTUBE
        else:
            raise UnsupportedPlatformError(f"Unsupported platform: {domain}")

    async def extract_content(self, url: str) -> ExtractedContent:
        """Extract content from URL."""
        start_time = time.time()

        try:
            platform = self._detect_platform(url)
            content = await self._extract_with_playwright(url, platform)

            # Calculate extraction time
            extraction_time = time.time() - start_time
            content.extraction_time = extraction_time

            return content

        except Exception as e:
            # Re-raise known exceptions without wrapping
            if isinstance(e, (UnsupportedPlatformError, TimeoutError)):
                raise
            # Wrap other exceptions
            raise Exception(f"Content extraction failed: {str(e)}")

    async def _extract_with_playwright(
        self, url: str, platform: PlatformType
    ) -> ExtractedContent:
        """Extract content using Playwright."""
        try:
            # Import playwright here to avoid import errors if not installed
            from playwright.async_api import TimeoutError as PlaywrightTimeoutError
            from playwright.async_api import async_playwright
        except ImportError:
            # Fallback to mock implementation if Playwright not available
            return await self._extract_mock_content(url, platform)

        try:
            async with async_playwright() as p:
                # Launch browser with mobile user agent for better Instagram support
                browser = await p.chromium.launch(
                    headless=True,
                    args=[
                        "--no-sandbox",
                        "--disable-setuid-sandbox",
                        "--disable-dev-shm-usage",
                        "--disable-accelerated-2d-canvas",
                        "--no-first-run",
                        "--no-zygote",
                        "--disable-gpu",
                    ],
                )

                context = await browser.new_context(
                    user_agent="Mozilla/5.0 (iPhone; CPU iPhone OS 14_7_1 like Mac OS X) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/14.1.2 Mobile/15E148 Safari/604.1",
                    viewport={"width": 375, "height": 812},
                )

                page = await context.new_page()

                # Set timeout for page operations
                page.set_default_timeout(self.timeout * 1000)  # Convert to milliseconds

                try:
                    # Navigate to URL
                    await page.goto(
                        url, wait_until="networkidle", timeout=self.timeout * 1000
                    )

                    # Extract metadata based on platform
                    if platform == PlatformType.INSTAGRAM:
                        metadata = await self._extract_instagram_metadata(page)
                    elif platform == PlatformType.NAVER_BLOG:
                        metadata = await self._extract_naver_blog_metadata(page)
                    elif platform == PlatformType.YOUTUBE:
                        metadata = await self._extract_youtube_metadata(page)
                    else:
                        metadata = await self._extract_generic_metadata(page)

                    return ExtractedContent(
                        url=url, platform=platform, metadata=metadata
                    )

                finally:
                    await browser.close()

        except PlaywrightTimeoutError:
            raise TimeoutError(f"Content extraction timed out for URL: {url}")
        except Exception as e:
            raise ContentExtractionError(
                f"Failed to extract content from {url}: {str(e)}"
            )

    async def _extract_mock_content(
        self, url: str, platform: PlatformType
    ) -> ExtractedContent:
        """Mock content extraction for testing/fallback."""
        # Simulate processing time
        await asyncio.sleep(0.1)

        # Use URL hash to select different mock scenarios for variety
        import hashlib

        url_hash = int(hashlib.md5(url.encode()).hexdigest(), 16) % 5

        # Return mock content based on platform
        if platform == PlatformType.INSTAGRAM:
            mock_scenarios = [
                # Scenario 1: ì„±ìˆ˜ë™ ì¹´íŽ˜
                ContentMetadata(
                    title="ì„±ìˆ˜ë™ ì¹´íŽ˜ ì˜¤ì•„ì‹œìŠ¤ì—ì„œ ë¸ŒëŸ°ì¹˜ ë¨¹ì—ˆì–´ìš”",
                    description="ì„±ìˆ˜ë™ì— ìžˆëŠ” ë¶ìœ ëŸ½ ê°ì„± ì¹´íŽ˜ìž…ë‹ˆë‹¤. ë£¨í”„íƒ‘ë„ ìžˆê³  ë¸ŒëŸ°ì¹˜ ë©”ë‰´ê°€ ì •ë§ ë§›ìžˆì–´ìš”! ì—ê·¸ë² ë„¤ë”•íŠ¸ ì¶”ì²œí•©ë‹ˆë‹¤.",
                    images=[],
                    location="ì„œìš¸ ì„±ë™êµ¬ ì„±ìˆ˜ë™",
                    hashtags=["#ì„±ìˆ˜ë™ì¹´íŽ˜", "#ë¸ŒëŸ°ì¹˜ë§›ì§‘", "#ë£¨í”„íƒ‘ì¹´íŽ˜", "#ê°ì„±ì¹´íŽ˜", "#ë°ì´íŠ¸ì½”ìŠ¤"],
                ),
                # Scenario 2: ê°•ë‚¨ ê³ ê¹ƒì§‘
                ContentMetadata(
                    title="ê°•ë‚¨ ìˆ™ì„± í•œìš° ë§›ì§‘ ë°œê²¬!",
                    description="ê°•ë‚¨ì—­ ê·¼ì²˜ í•œìš° ë§›ì§‘ì´ì—ìš”. 60ì¼ ìˆ™ì„± í•œìš°ë¼ ì§„ì§œ ë¶€ë“œëŸ½ê³  ë§›ìžˆì–´ìš”. ê°€ê²©ì€ ì¢€ ìžˆì§€ë§Œ íŠ¹ë³„í•œ ë‚  ì¶”ì²œ!",
                    images=[],
                    location="ì„œìš¸ ê°•ë‚¨êµ¬ ì—­ì‚¼ë™",
                    hashtags=["#ê°•ë‚¨ë§›ì§‘", "#í•œìš°ë§›ì§‘", "#ìˆ™ì„±í•œìš°", "#ê³ ê¸°ì§‘ì¶”ì²œ", "#ê¸°ë…ì¼ë§›ì§‘"],
                ),
                # Scenario 3: í™ëŒ€ ë””ì €íŠ¸ ì¹´íŽ˜
                ContentMetadata(
                    title="í™ëŒ€ í‹°ë¼ë¯¸ìˆ˜ ë§›ì§‘ ðŸ°",
                    description="í™ëŒ€ì—ì„œ ì œì¼ ë§›ìžˆëŠ” í‹°ë¼ë¯¸ìˆ˜! ì»¤í”¼ë„ ì§„í•˜ê³  ì¢‹ì•„ìš”. ì¸í…Œë¦¬ì–´ë„ ì˜ˆë»ì„œ ì‚¬ì§„ ì°ê¸° ì¢‹ìŒ",
                    images=[],
                    location="ì„œìš¸ ë§ˆí¬êµ¬ í™ëŒ€ìž…êµ¬",
                    hashtags=["#í™ëŒ€ì¹´íŽ˜", "#í‹°ë¼ë¯¸ìˆ˜ë§›ì§‘", "#ë””ì €íŠ¸ì¹´íŽ˜", "#í™ëŒ€ë°ì´íŠ¸", "#ì¹´íŽ˜íˆ¬ì–´"],
                ),
                # Scenario 4: ì´íƒœì› ì´íƒˆë¦¬ì•ˆ ë ˆìŠ¤í† ëž‘
                ContentMetadata(
                    title="ì´íƒœì› íŒŒìŠ¤íƒ€ ë§›ì§‘ ì¶”ì²œ",
                    description="ì •í†µ ì´íƒˆë¦¬ì•ˆ ë ˆìŠ¤í† ëž‘! íŒŒìŠ¤íƒ€ ë©´ì„ ì§ì ‘ ë§Œë“¤ì–´ì„œ ì«„ê¹ƒí•˜ê³  ì†ŒìŠ¤ë„ ì§„ì§œ ë§›ìžˆì–´ìš”. ì™€ì¸ë„ ìž˜ ì–´ìš¸ë¦¼",
                    images=[],
                    location="ì„œìš¸ ìš©ì‚°êµ¬ ì´íƒœì›ë™",
                    hashtags=["#ì´íƒœì›ë§›ì§‘", "#íŒŒìŠ¤íƒ€ë§›ì§‘", "#ì´íƒˆë¦¬ì•ˆë ˆìŠ¤í† ëž‘", "#ë°ì´íŠ¸ì½”ìŠ¤", "#ì™€ì¸ë§›ì§‘"],
                ),
                # Scenario 5: ì—¬ì˜ë„ ì˜¤í”¼ìŠ¤ ì¹´íŽ˜
                ContentMetadata(
                    title="ì—¬ì˜ë„ ì§ìž¥ì¸ í•«í”Œ ì¹´íŽ˜",
                    description="ì—¬ì˜ë„ì—ì„œ ì¼í•˜ëŠ” ì§ìž¥ì¸ì´ë¼ë©´ ì—¬ê¸°! ì•„ë©”ë¦¬ì¹´ë…¸ë„ ë§›ìžˆê³  ë¸ŒëŸ°ì¹˜ë„ ê´œì°®ì•„ìš”. ì ì‹¬ì‹œê°„ì—” ì‚¬ëžŒ ë§Žìœ¼ë‹ˆ ì°¸ê³ ",
                    images=[],
                    location="ì„œìš¸ ì˜ë“±í¬êµ¬ ì—¬ì˜ë„ë™",
                    hashtags=["#ì—¬ì˜ë„ì¹´íŽ˜", "#ì§ìž¥ì¸ì¹´íŽ˜", "#ë¸ŒëŸ°ì¹˜ì¹´íŽ˜", "#ì˜¤í”¼ìŠ¤ë§›ì§‘", "#ì—¬ì˜ë„í•«í”Œ"],
                ),
            ]
            metadata = mock_scenarios[url_hash]
        elif platform == PlatformType.NAVER_BLOG:
            metadata = ContentMetadata(
                title="ì„œìš¸ í•«í”Œ ë§›ì§‘ ë¦¬ìŠ¤íŠ¸",
                description="ì„œìš¸ì—ì„œ ê¼­ ê°€ë´ì•¼ í•  ë§›ì§‘ë“¤ì„ ì •ë¦¬í•´ë´¤ì–´ìš”. ê°•ë‚¨, í™ëŒ€, ì„±ìˆ˜ë™ ë“± ì§€ì—­ë³„ë¡œ ì¶”ì²œ ë§›ì§‘ì„ ì†Œê°œí•©ë‹ˆë‹¤.",
                images=[],
                location="ì„œìš¸",
                hashtags=["#ì„œìš¸ë§›ì§‘", "#ë§›ì§‘ì¶”ì²œ", "#ë§›ì§‘íƒë°©"],
            )
        elif platform == PlatformType.YOUTUBE:
            metadata = ContentMetadata(
                title="ì„œìš¸ ì¹´íŽ˜ ë¸Œì´ë¡œê·¸ | ì„±ìˆ˜ë™ ì¹´íŽ˜íˆ¬ì–´",
                description="ì„±ìˆ˜ë™ì—ì„œ ê°€ìž¥ í•«í•œ ì¹´íŽ˜ë“¤ì„ ë°©ë¬¸í•´ë´¤ì–´ìš”! ì¸í…Œë¦¬ì–´ ì˜ˆì˜ê³  ì»¤í”¼ ë§›ìžˆëŠ” ê³³ë“¤ë§Œ ê³¨ë¼ì„œ ê°”ìŠµë‹ˆë‹¤.",
                images=[],
                hashtags=["#ì„±ìˆ˜ë™ì¹´íŽ˜", "#ì¹´íŽ˜ë¸Œì´ë¡œê·¸", "#ì„œìš¸ì¹´íŽ˜íˆ¬ì–´"],
            )
        else:
            metadata = ContentMetadata(
                title="ë§›ì§‘ ì •ë³´", description="ë§›ì§‘ ê´€ë ¨ ì½˜í…ì¸ ìž…ë‹ˆë‹¤."
            )

        return ExtractedContent(url=url, platform=platform, metadata=metadata)

    async def _extract_instagram_metadata(self, page) -> ContentMetadata:
        """Extract metadata from Instagram page."""
        try:
            # Wait for content to load
            await page.wait_for_selector("article", timeout=10000)

            # Extract title/caption
            title = None
            description = None
            try:
                # Try to get the main post text
                caption_element = await page.query_selector(
                    'article div[data-testid="post-caption"] span'
                )
                if caption_element:
                    caption_text = await caption_element.text_content()
                    if caption_text:
                        lines = caption_text.strip().split("\n")
                        title = lines[0][:100] if lines else None  # First line as title
                        description = caption_text[:500]  # Full text as description
            except:
                pass

            # Extract images
            images = []
            try:
                img_elements = await page.query_selector_all("article img")
                for img in img_elements:
                    src = await img.get_attribute("src")
                    if src and "instagram" in src:
                        images.append(src)
            except:
                pass

            # Extract hashtags
            hashtags = []
            if description:
                hashtags = re.findall(r"#\w+", description)

            # Try to extract location
            location = None
            try:
                location_element = await page.query_selector(
                    '[data-testid="location-link"]'
                )
                if location_element:
                    location = await location_element.text_content()
            except:
                pass

            return ContentMetadata(
                title=title or "Instagram Post",
                description=description or "Instagram content",
                images=images,
                location=location,
                hashtags=hashtags,
            )

        except Exception:
            # Return basic metadata if detailed extraction fails
            return ContentMetadata(
                title="Instagram Post",
                description="Content extraction partially failed",
                images=[],
                hashtags=[],
            )

    async def _extract_naver_blog_metadata(self, page) -> ContentMetadata:
        """Extract metadata from Naver Blog page."""
        try:
            # Wait for content to load
            await page.wait_for_selector(
                ".se-main-container, .post-view, #postViewArea", timeout=10000
            )

            # Extract title
            title = None
            try:
                title_selectors = [
                    ".se-title-text",
                    ".post_title",
                    ".title_area h3",
                    "h3.tit_h3",
                ]
                for selector in title_selectors:
                    title_element = await page.query_selector(selector)
                    if title_element:
                        title = await title_element.text_content()
                        break
            except:
                pass

            # Extract description/content
            description = None
            try:
                content_selectors = [
                    ".se-main-container",
                    ".post-view .content",
                    "#postViewArea",
                ]
                for selector in content_selectors:
                    content_element = await page.query_selector(selector)
                    if content_element:
                        content_text = await content_element.text_content()
                        if content_text:
                            description = content_text.strip()[
                                :500
                            ]  # Limit to 500 chars
                            break
            except:
                pass

            # Extract images
            images = []
            try:
                img_elements = await page.query_selector_all("img")
                for img in img_elements:
                    src = await img.get_attribute("src")
                    if src and (
                        "blogfiles.naver.net" in src or "blog.naver.com" in src
                    ):
                        images.append(src)
            except:
                pass

            # Extract hashtags from content
            hashtags = []
            if description:
                hashtags = re.findall(r"#\w+", description)

            return ContentMetadata(
                title=title or "Naver Blog Post",
                description=description or "Blog content",
                images=images,
                hashtags=hashtags,
            )

        except Exception:
            return ContentMetadata(
                title="Naver Blog Post",
                description="Content extraction partially failed",
                images=[],
                hashtags=[],
            )

    async def _extract_youtube_metadata(self, page) -> ContentMetadata:
        """Extract metadata from YouTube page."""
        try:
            # Wait for content to load
            await page.wait_for_selector(
                "#watch7-content, ytd-watch-flexy", timeout=10000
            )

            # Extract title
            title = None
            try:
                title_selectors = [
                    "h1.title yt-formatted-string",
                    "#watch7-headline h1",
                    "h1.ytd-video-primary-info-renderer",
                ]
                for selector in title_selectors:
                    title_element = await page.query_selector(selector)
                    if title_element:
                        title = await title_element.text_content()
                        break
            except:
                pass

            # Extract description
            description = None
            try:
                desc_selectors = [
                    "#watch7-description-text",
                    "ytd-expandable-video-description-body-renderer",
                    "#description",
                ]
                for selector in desc_selectors:
                    desc_element = await page.query_selector(selector)
                    if desc_element:
                        desc_text = await desc_element.text_content()
                        if desc_text:
                            description = desc_text.strip()[:500]
                            break
            except:
                pass

            # Extract thumbnail
            images = []
            try:
                # Try to get video thumbnail
                video_id_match = re.search(r"(?:v=|/)([0-9A-Za-z_-]{11})", page.url)
                if video_id_match:
                    video_id = video_id_match.group(1)
                    images.append(
                        f"https://img.youtube.com/vi/{video_id}/maxresdefault.jpg"
                    )
            except:
                pass

            return ContentMetadata(
                title=title or "YouTube Video",
                description=description or "YouTube video content",
                images=images,
                hashtags=[],
            )

        except Exception:
            return ContentMetadata(
                title="YouTube Video",
                description="Content extraction partially failed",
                images=[],
                hashtags=[],
            )

    async def _extract_generic_metadata(self, page) -> ContentMetadata:
        """Extract generic metadata using meta tags."""
        try:
            # Extract title
            title = None
            try:
                title = await page.title()
            except:
                pass

            # Extract meta description
            description = None
            try:
                desc_element = await page.query_selector('meta[name="description"]')
                if desc_element:
                    description = await desc_element.get_attribute("content")
            except:
                pass

            # Extract Open Graph images
            images = []
            try:
                og_img_elements = await page.query_selector_all(
                    'meta[property="og:image"]'
                )
                for img in og_img_elements:
                    src = await img.get_attribute("content")
                    if src:
                        images.append(src)
            except:
                pass

            return ContentMetadata(
                title=title or "Web Content",
                description=description or "Generic web content",
                images=images,
                hashtags=[],
            )

        except Exception:
            return ContentMetadata(
                title="Web Content",
                description="Content extraction failed",
                images=[],
                hashtags=[],
            )
