"""
ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ TDD í…ŒìŠ¤íŠ¸

API ì‘ë‹µ ìµœì í™” ë° ì„±ëŠ¥ íŠœë‹ ëŒ€ì‹œë³´ë“œë¥¼ ìœ„í•œ TDD í…ŒìŠ¤íŠ¸ë¥¼ ì •ì˜í•©ë‹ˆë‹¤.
"""
# from fastapi.testclient import TestClient
# from fastapi import status
import time

# import pytest
from datetime import datetime, timedelta
from typing import Any, Dict, List


class TestPerformanceMetricsCollector:
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸° í…ŒìŠ¤íŠ¸"""

    def test_api_response_time_measurement(self):
        """API ì‘ë‹µ ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸"""
        # Given: ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘ê¸°
        metrics_collector = {}

        def measure_response_time(endpoint: str, method: str):
            """ì‘ë‹µ ì‹œê°„ ì¸¡ì • ë°ì½”ë ˆì´í„°"""

            def decorator(func):
                def wrapper(*args, **kwargs):
                    start_time = time.perf_counter()
                    try:
                        result = func(*args, **kwargs)
                        return result
                    finally:
                        end_time = time.perf_counter()
                        duration_ms = (end_time - start_time) * 1000

                        key = f"{method}_{endpoint}"
                        if key not in metrics_collector:
                            metrics_collector[key] = []
                        metrics_collector[key].append(duration_ms)

                return wrapper

            return decorator

        # When: API ì—”ë“œí¬ì¸íŠ¸ í˜¸ì¶œ ì‹œë®¬ë ˆì´ì…˜
        @measure_response_time("/api/v1/places", "GET")
        def get_places():
            time.sleep(0.01)  # 10ms ì‹œë®¬ë ˆì´ì…˜
            return {"places": []}

        # ì—¬ëŸ¬ ë²ˆ í˜¸ì¶œ
        for _ in range(5):
            get_places()

        # Then: ì‘ë‹µ ì‹œê°„ì´ ìˆ˜ì§‘ë¨
        assert "GET_/api/v1/places" in metrics_collector
        response_times = metrics_collector["GET_/api/v1/places"]
        assert len(response_times) == 5
        assert all(rt >= 10.0 for rt in response_times)  # ìµœì†Œ 10ms
        assert all(rt < 50.0 for rt in response_times)  # ìµœëŒ€ 50ms

        print("âœ… API ì‘ë‹µ ì‹œê°„ ì¸¡ì • í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_cache_performance_tracking(self):
        """ìºì‹œ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸"""
        # Given: ìºì‹œ ì„±ëŠ¥ ì¶”ì ê¸°
        cache_metrics = {
            "hit_count": 0,
            "miss_count": 0,
            "total_time": 0,
            "operations": [],
        }

        def track_cache_operation(operation: str, hit: bool, duration_ms: float):
            """ìºì‹œ ì—°ì‚° ì¶”ì """
            cache_metrics["operations"].append(
                {
                    "operation": operation,
                    "hit": hit,
                    "duration_ms": duration_ms,
                    "timestamp": datetime.now().isoformat(),
                }
            )

            if hit:
                cache_metrics["hit_count"] += 1
            else:
                cache_metrics["miss_count"] += 1

            cache_metrics["total_time"] += duration_ms

        # When: ìºì‹œ ì—°ì‚° ì‹œë®¬ë ˆì´ì…˜
        track_cache_operation("GET", True, 0.5)  # ìºì‹œ íˆíŠ¸ - ë¹ ë¦„
        track_cache_operation("GET", False, 25.0)  # ìºì‹œ ë¯¸ìŠ¤ - ëŠë¦¼
        track_cache_operation("GET", True, 0.3)  # ìºì‹œ íˆíŠ¸ - ë¹ ë¦„
        track_cache_operation("SET", True, 1.2)  # ìºì‹œ ì„¤ì •

        # Then: ìºì‹œ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ìˆ˜ì§‘
        assert cache_metrics["hit_count"] == 3
        assert cache_metrics["miss_count"] == 1
        assert len(cache_metrics["operations"]) == 4

        hit_rate = cache_metrics["hit_count"] / (
            cache_metrics["hit_count"] + cache_metrics["miss_count"]
        )
        assert hit_rate == 0.75  # 75% ì ì¤‘ë¥ 

        avg_time = cache_metrics["total_time"] / len(cache_metrics["operations"])
        assert avg_time == (0.5 + 25.0 + 0.3 + 1.2) / 4

        print("âœ… ìºì‹œ ì„±ëŠ¥ ì¶”ì  í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_database_query_performance(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì • í…ŒìŠ¤íŠ¸"""
        # Given: DB ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        query_metrics = []

        def monitor_query_performance(
            query: str, execution_time: float, rows_affected: int
        ):
            """ì¿¼ë¦¬ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
            query_metrics.append(
                {
                    "query": query[:50] + "..." if len(query) > 50 else query,
                    "execution_time_ms": execution_time,
                    "rows_affected": rows_affected,
                    "timestamp": datetime.now().isoformat(),
                    "slow_query": execution_time > 100,  # 100ms ì´ìƒì€ ëŠë¦° ì¿¼ë¦¬
                }
            )

        # When: ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        monitor_query_performance("SELECT * FROM places WHERE id = ?", 2.5, 1)
        monitor_query_performance(
            "SELECT * FROM places WHERE category LIKE '%restaurant%'", 150.0, 500
        )
        monitor_query_performance("INSERT INTO user_activities (...)", 5.0, 1)
        monitor_query_performance(
            "UPDATE places SET updated_at = NOW() WHERE ...", 45.0, 20
        )

        # Then: ì¿¼ë¦¬ ì„±ëŠ¥ ë¶„ì„
        assert len(query_metrics) == 4

        slow_queries = [q for q in query_metrics if q["slow_query"]]
        assert len(slow_queries) == 1
        assert "SELECT * FROM places WHERE category LIKE" in slow_queries[0]["query"]

        fast_queries = [q for q in query_metrics if not q["slow_query"]]
        assert len(fast_queries) == 3

        avg_time = sum(q["execution_time_ms"] for q in query_metrics) / len(
            query_metrics
        )
        assert avg_time == (2.5 + 150.0 + 5.0 + 45.0) / 4

        print("âœ… ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬ ì„±ëŠ¥ ì¸¡ì • í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestPerformanceDashboard:
    """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸"""

    def test_real_time_metrics_display(self):
        """ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í‘œì‹œ í…ŒìŠ¤íŠ¸"""
        # Given: ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ ëŒ€ì‹œë³´ë“œ
        dashboard_data = {"current_metrics": {}, "history": [], "alerts": []}

        def update_dashboard_metrics(metrics: Dict[str, Any]):
            """ëŒ€ì‹œë³´ë“œ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
            timestamp = datetime.now().isoformat()
            dashboard_data["current_metrics"] = {**metrics, "timestamp": timestamp}

            # íˆìŠ¤í† ë¦¬ì— ì¶”ê°€ (ìµœê·¼ 100ê°œë§Œ ìœ ì§€)
            dashboard_data["history"].append({**metrics, "timestamp": timestamp})

            if len(dashboard_data["history"]) > 100:
                dashboard_data["history"] = dashboard_data["history"][-100:]

            # ì„ê³„ê°’ ì²´í¬ ë° ì•Œë¦¼
            if metrics.get("response_time_p95", 0) > 1000:  # 1ì´ˆ ì´ˆê³¼
                dashboard_data["alerts"].append(
                    {
                        "type": "high_response_time",
                        "message": f"P95 response time: {metrics['response_time_p95']}ms",
                        "timestamp": timestamp,
                        "severity": "warning",
                    }
                )

        # When: ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
        test_metrics = {
            "active_connections": 45,
            "requests_per_second": 120,
            "response_time_avg": 250,
            "response_time_p95": 800,
            "cache_hit_rate": 0.85,
            "error_rate": 0.02,
        }

        update_dashboard_metrics(test_metrics)

        # Then: ëŒ€ì‹œë³´ë“œ ë°ì´í„° í™•ì¸
        assert dashboard_data["current_metrics"]["active_connections"] == 45
        assert dashboard_data["current_metrics"]["requests_per_second"] == 120
        assert dashboard_data["current_metrics"]["cache_hit_rate"] == 0.85
        assert "timestamp" in dashboard_data["current_metrics"]

        assert len(dashboard_data["history"]) == 1
        assert len(dashboard_data["alerts"]) == 0  # ì„ê³„ê°’ ë¯¸ì´ˆê³¼ë¡œ ì•Œë¦¼ ì—†ìŒ

        # When: ì„ê³„ê°’ ì´ˆê³¼ ë©”íŠ¸ë¦­
        alert_metrics = {**test_metrics, "response_time_p95": 1500}  # ì„ê³„ê°’ ì´ˆê³¼

        update_dashboard_metrics(alert_metrics)

        # Then: ì•Œë¦¼ ìƒì„± í™•ì¸
        assert len(dashboard_data["alerts"]) == 1
        assert dashboard_data["alerts"][0]["type"] == "high_response_time"
        assert dashboard_data["alerts"][0]["severity"] == "warning"

        print("âœ… ì‹¤ì‹œê°„ ë©”íŠ¸ë¦­ í‘œì‹œ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_performance_trend_analysis(self):
        """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸"""
        # Given: ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ê¸°
        metrics_history = []

        def add_metrics_snapshot(
            response_time: float, cache_hit_rate: float, error_rate: float
        ):
            """ë©”íŠ¸ë¦­ ìŠ¤ëƒ…ìƒ· ì¶”ê°€"""
            metrics_history.append(
                {
                    "timestamp": datetime.now(),
                    "response_time": response_time,
                    "cache_hit_rate": cache_hit_rate,
                    "error_rate": error_rate,
                }
            )

        def analyze_performance_trend(hours: int = 1) -> Dict[str, Any]:
            """ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„"""
            cutoff_time = datetime.now() - timedelta(hours=hours)
            recent_metrics = [
                m for m in metrics_history if m["timestamp"] >= cutoff_time
            ]

            if not recent_metrics:
                return {"trend": "no_data"}

            # í‰ê·  ê³„ì‚°
            avg_response_time = sum(m["response_time"] for m in recent_metrics) / len(
                recent_metrics
            )
            avg_cache_hit_rate = sum(m["cache_hit_rate"] for m in recent_metrics) / len(
                recent_metrics
            )
            avg_error_rate = sum(m["error_rate"] for m in recent_metrics) / len(
                recent_metrics
            )

            # íŠ¸ë Œë“œ ê³„ì‚° (ì²« ë²ˆì§¸ vs ë§ˆì§€ë§‰ ê°’ ë¹„êµ)
            if len(recent_metrics) >= 2:
                first = recent_metrics[0]
                last = recent_metrics[-1]

                response_time_trend = (
                    "improving"
                    if last["response_time"] < first["response_time"]
                    else (
                        "degrading"
                        if last["response_time"] > first["response_time"]
                        else "stable"
                    )
                )
                cache_trend = (
                    "improving"
                    if last["cache_hit_rate"] > first["cache_hit_rate"]
                    else (
                        "degrading"
                        if last["cache_hit_rate"] < first["cache_hit_rate"]
                        else "stable"
                    )
                )
                error_trend = (
                    "improving"
                    if last["error_rate"] < first["error_rate"]
                    else (
                        "degrading"
                        if last["error_rate"] > first["error_rate"]
                        else "stable"
                    )
                )
            else:
                response_time_trend = cache_trend = error_trend = "stable"

            return {
                "period_hours": hours,
                "data_points": len(recent_metrics),
                "averages": {
                    "response_time": avg_response_time,
                    "cache_hit_rate": avg_cache_hit_rate,
                    "error_rate": avg_error_rate,
                },
                "trends": {
                    "response_time": response_time_trend,
                    "cache_hit_rate": cache_trend,
                    "error_rate": error_trend,
                },
            }

        # When: ì‹œê°„ íë¦„ì— ë”°ë¥¸ ë©”íŠ¸ë¦­ ì¶”ê°€
        # ì„±ëŠ¥ì´ ì ì§„ì ìœ¼ë¡œ ê°œì„ ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤
        add_metrics_snapshot(300.0, 0.70, 0.05)  # ì´ˆê¸° - ì„±ëŠ¥ ë‚˜ì¨
        add_metrics_snapshot(250.0, 0.75, 0.04)  # ê°œì„  ì¤‘
        add_metrics_snapshot(200.0, 0.80, 0.03)  # ë” ê°œì„ 
        add_metrics_snapshot(180.0, 0.85, 0.02)  # ìµœì¢… - ì„±ëŠ¥ ì¢‹ìŒ

        analysis = analyze_performance_trend(1)

        # Then: íŠ¸ë Œë“œ ë¶„ì„ ê²°ê³¼ í™•ì¸
        assert analysis["data_points"] == 4
        # íŠ¸ë Œë“œ ë¶„ì„ì€ ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ ê°’ì„ ë¹„êµ
        # 300->180 (improving), 0.70->0.85 (improving), 0.05->0.02 (improving)
        assert analysis["trends"]["response_time"] == "improving"
        assert analysis["trends"]["cache_hit_rate"] == "improving"
        assert analysis["trends"]["error_rate"] == "improving"

        import math

        assert analysis["averages"]["response_time"] == (300 + 250 + 200 + 180) / 4
        assert analysis["averages"]["cache_hit_rate"] == (0.70 + 0.75 + 0.80 + 0.85) / 4
        assert math.isclose(
            analysis["averages"]["error_rate"], (0.05 + 0.04 + 0.03 + 0.02) / 4
        )

        print("âœ… ì„±ëŠ¥ íŠ¸ë Œë“œ ë¶„ì„ í…ŒìŠ¤íŠ¸ í†µê³¼")


class TestPerformanceOptimization:
    """ì„±ëŠ¥ ìµœì í™” í…ŒìŠ¤íŠ¸"""

    def test_slow_query_detection(self):
        """ëŠë¦° ì¿¼ë¦¬ ê°ì§€ í…ŒìŠ¤íŠ¸"""
        # Given: ëŠë¦° ì¿¼ë¦¬ ê°ì§€ ì‹œìŠ¤í…œ
        slow_queries = []
        query_threshold_ms = 100

        def detect_slow_query(
            query: str, execution_time_ms: float, params: Dict = None
        ):
            """ëŠë¦° ì¿¼ë¦¬ ê°ì§€"""
            if execution_time_ms > query_threshold_ms:
                slow_queries.append(
                    {
                        "query": query,
                        "execution_time_ms": execution_time_ms,
                        "params": params or {},
                        "detected_at": datetime.now().isoformat(),
                        "optimization_suggestions": generate_optimization_suggestions(
                            query, execution_time_ms
                        ),
                    }
                )

        def generate_optimization_suggestions(query: str, time_ms: float) -> List[str]:
            """ìµœì í™” ì œì•ˆ ìƒì„±"""
            suggestions = []

            if "SELECT *" in query.upper():
                suggestions.append("Avoid SELECT *, specify only needed columns")

            if "LIKE" in query.upper() and "%" in query:
                suggestions.append(
                    "Consider full-text search or proper indexing for LIKE patterns"
                )

            if time_ms > 500:
                suggestions.append(
                    "Query takes too long, consider query restructuring or caching"
                )

            if "JOIN" in query.upper() and time_ms > 200:
                suggestions.append(
                    "Optimize JOIN conditions and ensure proper indexing"
                )

            return suggestions

        # When: ë‹¤ì–‘í•œ ì¿¼ë¦¬ ì‹œë®¬ë ˆì´ì…˜
        detect_slow_query(
            "SELECT id, name FROM places WHERE category = ?",
            50,
            {"category": "restaurant"},
        )
        detect_slow_query(
            "SELECT * FROM places WHERE name LIKE '%pizza%'", 250, {"pattern": "pizza"}
        )
        detect_slow_query(
            "SELECT p.*, r.* FROM places p JOIN reviews r ON p.id = r.place_id", 350
        )
        detect_slow_query(
            "SELECT COUNT(*) FROM user_activities WHERE created_at > ?", 80
        )

        # Then: ëŠë¦° ì¿¼ë¦¬ ê°ì§€ ë° ìµœì í™” ì œì•ˆ
        assert len(slow_queries) == 2  # 250ms, 350ms ì¿¼ë¦¬

        like_query = next(q for q in slow_queries if "LIKE" in q["query"])
        suggestions = like_query["optimization_suggestions"]
        assert any("Avoid SELECT *" in suggestion for suggestion in suggestions)
        assert any("full-text search" in suggestion for suggestion in suggestions)

        join_query = next(q for q in slow_queries if "JOIN" in q["query"])
        assert any(
            "Optimize JOIN conditions" in suggestion
            for suggestion in join_query["optimization_suggestions"]
        )

        print("âœ… ëŠë¦° ì¿¼ë¦¬ ê°ì§€ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_api_endpoint_performance_ranking(self):
        """API ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ìˆœìœ„ í…ŒìŠ¤íŠ¸"""
        # Given: ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ì¶”ì 
        endpoint_stats = {}

        def track_endpoint_performance(
            endpoint: str, method: str, response_time_ms: float, status_code: int
        ):
            """ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ì¶”ì """
            key = f"{method} {endpoint}"

            if key not in endpoint_stats:
                endpoint_stats[key] = {
                    "total_requests": 0,
                    "total_time": 0,
                    "success_count": 0,
                    "error_count": 0,
                    "min_time": float("inf"),
                    "max_time": 0,
                }

            stats = endpoint_stats[key]
            stats["total_requests"] += 1
            stats["total_time"] += response_time_ms

            if 200 <= status_code < 400:
                stats["success_count"] += 1
            else:
                stats["error_count"] += 1

            stats["min_time"] = min(stats["min_time"], response_time_ms)
            stats["max_time"] = max(stats["max_time"], response_time_ms)

        def get_performance_ranking() -> List[Dict[str, Any]]:
            """ì„±ëŠ¥ ìˆœìœ„ ì¡°íšŒ"""
            ranking = []

            for endpoint, stats in endpoint_stats.items():
                avg_time = stats["total_time"] / stats["total_requests"]
                success_rate = stats["success_count"] / stats["total_requests"]

                ranking.append(
                    {
                        "endpoint": endpoint,
                        "avg_response_time": avg_time,
                        "success_rate": success_rate,
                        "total_requests": stats["total_requests"],
                        "min_time": stats["min_time"],
                        "max_time": stats["max_time"],
                        "performance_score": success_rate
                        * (1000 / max(avg_time, 1)),  # ë†’ì„ìˆ˜ë¡ ì¢‹ìŒ
                    }
                )

            # ì„±ëŠ¥ ì ìˆ˜ë¡œ ì •ë ¬ (ë†’ì€ ìˆœ)
            return sorted(ranking, key=lambda x: x["performance_score"], reverse=True)

        # When: ë‹¤ì–‘í•œ ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ë°ì´í„°
        # ë¹ ë¥´ê³  ì•ˆì •ì ì¸ ì—”ë“œí¬ì¸íŠ¸
        for _ in range(10):
            track_endpoint_performance("/api/v1/health", "GET", 5.0, 200)

        # ë³´í†µ ì„±ëŠ¥ ì—”ë“œí¬ì¸íŠ¸
        for _ in range(20):
            track_endpoint_performance("/api/v1/places", "GET", 150.0, 200)

        # ëŠë¦¬ì§€ë§Œ ì•ˆì •ì ì¸ ì—”ë“œí¬ì¸íŠ¸
        for _ in range(5):
            track_endpoint_performance("/api/v1/search", "POST", 800.0, 200)

        # ë¹ ë¥´ì§€ë§Œ ì—ëŸ¬ê°€ ìˆëŠ” ì—”ë“œí¬ì¸íŠ¸
        for _ in range(8):
            track_endpoint_performance("/api/v1/users", "GET", 50.0, 200)
        for _ in range(2):
            track_endpoint_performance("/api/v1/users", "GET", 60.0, 500)

        ranking = get_performance_ranking()

        # Then: ì„±ëŠ¥ ìˆœìœ„ í™•ì¸
        assert len(ranking) == 4

        # ìµœê³  ì„±ëŠ¥ì€ health ì—”ë“œí¬ì¸íŠ¸ (ë¹ ë¥´ê³  100% ì„±ê³µ)
        best_performer = ranking[0]
        assert best_performer["endpoint"] == "GET /api/v1/health"
        assert best_performer["success_rate"] == 1.0
        assert best_performer["avg_response_time"] == 5.0

        # ì„±ëŠ¥ ì ìˆ˜ ìˆœì„œëŒ€ë¡œ ì •ë ¬ í™•ì¸
        scores = [ep["performance_score"] for ep in ranking]
        assert scores == sorted(scores, reverse=True)

        # ì—ëŸ¬ê°€ ìˆëŠ” ì—”ë“œí¬ì¸íŠ¸ì˜ ì„±ê³µë¥  í™•ì¸
        users_endpoint = next(ep for ep in ranking if "/users" in ep["endpoint"])
        assert users_endpoint["success_rate"] == 0.8  # 8/10

        print("âœ… API ì—”ë“œí¬ì¸íŠ¸ ì„±ëŠ¥ ìˆœìœ„ í…ŒìŠ¤íŠ¸ í†µê³¼")

    def test_automatic_scaling_recommendations(self):
        """ìë™ ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ í…ŒìŠ¤íŠ¸"""
        # Given: ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ ì‹œìŠ¤í…œ
        resource_metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "request_rate": [],
            "response_time": [],
        }

        def collect_resource_metrics(
            cpu: float, memory: float, request_rate: int, response_time: float
        ):
            """ë¦¬ì†ŒìŠ¤ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
            resource_metrics["cpu_usage"].append(cpu)
            resource_metrics["memory_usage"].append(memory)
            resource_metrics["request_rate"].append(request_rate)
            resource_metrics["response_time"].append(response_time)

        def generate_scaling_recommendations() -> List[Dict[str, Any]]:
            """ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
            recommendations = []

            if not resource_metrics["cpu_usage"]:
                return recommendations

            avg_cpu = sum(resource_metrics["cpu_usage"]) / len(
                resource_metrics["cpu_usage"]
            )
            avg_memory = sum(resource_metrics["memory_usage"]) / len(
                resource_metrics["memory_usage"]
            )
            avg_request_rate = sum(resource_metrics["request_rate"]) / len(
                resource_metrics["request_rate"]
            )
            avg_response_time = sum(resource_metrics["response_time"]) / len(
                resource_metrics["response_time"]
            )

            # CPU ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            if avg_cpu > 80:
                recommendations.append(
                    {
                        "type": "scale_up",
                        "resource": "cpu",
                        "current_usage": avg_cpu,
                        "recommended_action": "Increase CPU cores or scale out",
                        "priority": "high" if avg_cpu > 90 else "medium",
                    }
                )
            elif avg_cpu < 20:
                recommendations.append(
                    {
                        "type": "scale_down",
                        "resource": "cpu",
                        "current_usage": avg_cpu,
                        "recommended_action": "Consider reducing CPU allocation",
                        "priority": "low",
                    }
                )

            # ë©”ëª¨ë¦¬ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            if avg_memory > 85:
                recommendations.append(
                    {
                        "type": "scale_up",
                        "resource": "memory",
                        "current_usage": avg_memory,
                        "recommended_action": "Increase memory allocation",
                        "priority": "high" if avg_memory > 95 else "medium",
                    }
                )

            # ì‘ë‹µì‹œê°„ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­
            if avg_response_time > 1000:  # 1ì´ˆ ì´ˆê³¼
                recommendations.append(
                    {
                        "type": "performance",
                        "resource": "response_time",
                        "current_value": avg_response_time,
                        "recommended_action": "Scale out or optimize application performance",
                        "priority": "high",
                    }
                )

            return recommendations

        # When: ë‹¤ì–‘í•œ ë¦¬ì†ŒìŠ¤ ìƒí™© ì‹œë®¬ë ˆì´ì…˜

        # ì‹œë‚˜ë¦¬ì˜¤ 1: ì •ìƒ ìƒíƒœ
        for _ in range(5):
            collect_resource_metrics(45.0, 60.0, 100, 200.0)

        recommendations = generate_scaling_recommendations()
        assert len(recommendations) == 0  # ì •ìƒ ìƒíƒœë¼ ê¶Œì¥ì‚¬í•­ ì—†ìŒ

        # ì‹œë‚˜ë¦¬ì˜¤ 2: ë†’ì€ CPU ì‚¬ìš©ë¥ 
        resource_metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "request_rate": [],
            "response_time": [],
        }
        for _ in range(5):
            collect_resource_metrics(95.0, 70.0, 200, 300.0)

        recommendations = generate_scaling_recommendations()
        cpu_rec = next((r for r in recommendations if r["resource"] == "cpu"), None)
        assert cpu_rec is not None
        assert cpu_rec["type"] == "scale_up"
        assert cpu_rec["priority"] == "high"

        # ì‹œë‚˜ë¦¬ì˜¤ 3: ë†’ì€ ë©”ëª¨ë¦¬ + ëŠë¦° ì‘ë‹µì‹œê°„
        resource_metrics = {
            "cpu_usage": [],
            "memory_usage": [],
            "request_rate": [],
            "response_time": [],
        }
        for _ in range(5):
            collect_resource_metrics(60.0, 98.0, 150, 1500.0)

        recommendations = generate_scaling_recommendations()
        assert len(recommendations) == 2

        memory_rec = next(
            (r for r in recommendations if r["resource"] == "memory"), None
        )
        response_rec = next(
            (r for r in recommendations if r["resource"] == "response_time"), None
        )

        assert memory_rec["priority"] == "high"
        assert response_rec["priority"] == "high"
        assert "Scale out" in response_rec["recommended_action"]

        print("âœ… ìë™ ìŠ¤ì¼€ì¼ë§ ê¶Œì¥ì‚¬í•­ í…ŒìŠ¤íŠ¸ í†µê³¼")


def main():
    """ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("ğŸ¯ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ TDD í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 60)

    test_classes = [
        TestPerformanceMetricsCollector(),
        TestPerformanceDashboard(),
        TestPerformanceOptimization(),
    ]

    total_passed = 0
    total_failed = 0

    for test_instance in test_classes:
        class_name = test_instance.__class__.__name__
        print(f"\nğŸ“Š {class_name} í…ŒìŠ¤íŠ¸ ì‹¤í–‰")
        print("-" * 40)

        test_methods = [
            method for method in dir(test_instance) if method.startswith("test_")
        ]

        for method_name in test_methods:
            try:
                if hasattr(test_instance, "setup_method"):
                    test_instance.setup_method()

                test_method = getattr(test_instance, method_name)
                test_method()
                total_passed += 1
            except Exception as e:
                print(f"âŒ {method_name} ì‹¤íŒ¨: {e}")
                total_failed += 1

    print(f"\nğŸ“ˆ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   âœ… í†µê³¼: {total_passed}")
    print(f"   âŒ ì‹¤íŒ¨: {total_failed}")
    print(f"   ğŸ“Š ì „ì²´: {total_passed + total_failed}")

    if total_failed == 0:
        print("ğŸ† ëª¨ë“  ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ í…ŒìŠ¤íŠ¸ í†µê³¼!")
        return True
    else:
        print(f"âš ï¸ {total_failed}ê°œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
        return False


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)
